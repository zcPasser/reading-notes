[TOC]

# 经验误差与过拟合

## 术语

- 错误率

  分类错误的样本数占总样本的比例。

- 精度

  =`1 - 错误率`。

- 误差：指误差期望。

  学习器的实际预测输出与样本的真实输出之间的差异。

  在不同数据集上的误差也有区别，如下：

  - 训练误差 或 经验误差：训练集。
  - 泛化误差：新样本。

- P问题（P：polynomial）

  存在多项式时间算法的问题。(多项式时间即算法中O(n^2))

- NP问题（N：non-deterministic非确定性）

  不确定1个问题是否存在多项式时间内的算法，但可以在多项式时间内验证并得出这个问题的1个正确解。

  - P问题是NP问题的子集。

- P == NP ？

  千年问题：是否所有能在多项式时间内验证得出正确解的问题，都是具有多项式时间算法的问题。

  - 方法之一：问题约化

    即若问题B的算法可以解决问题A，称问题A可以约化成问题B。

    - 举例：B是二元一次方程求解，A是一元一次方程求解，而A添一个方程如`y=0`即可变为二元一次方程，B的规则性解法也适用A。
    - 约化具有传递性，一类问题不断约化，则一定会存在一个最大问题，解决该问题，即解决其下所有问题，有点类似数学归纳法中的一些概念。

- NPC问题（C：complete）

  NP完全问题，若所有NP问题都能在多项式时间内约化为它，则称该NP问题为NPC问题。

  - NPC问题是NP问题的子集。

- NPH问题（H：hard）

  NP难问题，不是1个NP问题，若所有NPC问题都可以在多项式时间内约化为它，则称之NPH（难）问题。

## 问题

学习器的目标是**`泛化误差最小`**，即在新样本上表现良好。

为尽可能降低泛化误差，学习器通过训练集尽可能学习使用**`所以潜在样本`**的**`普遍规律`**，以此判别新样本。

但学习器学习的好坏带来两个方面的问题，如下：

- 欠拟合 或 欠配

  学习器**学习效果差**，对于训练集的一般性质尚未学好，无法对新样本做出较准确的判断。

  - 举例：一堆树叶训练样本，只学习到了`绿色`等特征，对于树这个新样本，也判断其为树叶。

- 过拟合 或 过配 

  因为训练集只是样本集的子集，而学习器**学习效果过好**，可能会将特殊的即子集的特点当作一般的即新样本的普遍性质，导致泛化性能下降。

  - 举例：一堆有锯齿的树叶训练样本，由于这部分子集都带有锯齿特征，学习器当作所有树叶都具有该特征，对于新的边缘圆滑的树叶样本判断出其不是树叶。

## 解决

- 欠拟合

  提高学习能力，如在决策树学习中扩展分支、在神经网络学习中增加训练轮数epoch等。

- 过拟合

  较麻烦，无法彻底避免，只可缓解。

  即：机器学习面临的问题通常是NP难（NPH）甚至更难，而有效的算法必然是在多项式时间内运行完成，若可以彻底避免`过拟合`，则通过`经验误差`最小化就能获得最优解，即构造性证明了`P == NP`；因此，若认为`P != NP`，则过拟合不可避免。

  实际上，不同**学习算法**、不同**参数配置**会产生不同模型，即机器学习中`模型选择问题`。

  所以最佳方案：对**候选模型**的泛化误差进行**评估**，选择其中**泛化误差最小**的模型。而泛化误差无法直接获得，这则需要借助**测试集**进行模型评估和选择。

  

  