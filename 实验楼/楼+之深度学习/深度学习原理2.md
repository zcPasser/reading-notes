[TOC]

# 监督学习

## 逻辑回归

### 介绍

逻辑回归（Logistic Regression），又叫逻辑斯蒂回归，是机器学习中一种十分基础的**分类方法**。

### 线性可分和不可分

- 线性可分

二维平面中只使用一条直线就可以将样本分开。

三维空间内，则是通过一个平面将样本分开。

- 线性不可分

二维平面中无法只使用一条直线将样本分开。

### 使用线性回归分类

- 线性回归：通过拟合一条直线去预测更多的连续值。
- 线性回归可处理特殊情况下的分类问题。——处理**二分类问题**，即**0-1问题**。

- 具体处理

```python
# 可以定义：通过线性拟合函数  𝑓(𝑥)  计算的结果  𝑓(𝑥)>0.5  （靠近 1）代表 PASS，而  𝑓(𝑥)<=0.5  （靠近 0）代表不通过。
```

$$
\begin{array}{l}{f(x)>0.5=>y=1} \\ {f(x) \leq 0.5=>y=0}\end{array} \tag{1}
$$

```python
scores = [[1], [1], [2], [2], [3], [3], [3], [4], [4], [5],
          [6], [6], [7], [7], [8], [8], [8], [9], [9], [10]]
passed = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1]
# 首先，绘制数据集对应到二维平面中的散点图。
from matplotlib import pyplot as plt
%matplotlib inline

plt.scatter(scores, passed, color='r')
plt.xlabel("scores")
plt.ylabel("passed")
# 使用 scikit-learn 完成线性回归拟合的过程
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(scores, passed)
model.coef_, model.intercept_
# 接下来，将拟合直线绘制到散点图中。
import numpy as np

x = np.linspace(-2, 12, 100)

plt.plot(x, model.coef_[0] * x + model.intercept_)
plt.scatter(scores, passed, color='r')
plt.xlabel("scores")
plt.ylabel("passed")
```

预测存在错误，对于部分样例，预测错误，并不理想。

所以采用**逻辑回归**方式完成**0-1分类**问题。

### 逻辑回归实现0-1分类问题

- 使用sigmoid分布函数

$$
f(z)=\frac{1}{1+e^{-z}}
\tag{2}
$$

python实现

```python
def sigmoid(z):
    sigmoid = 1 / (1 + np.exp(-z))
    return sigmoid

# 案例
z = np.linspace(-12, 12, 100)  # 生成等间距 x 值方便绘图
plt.plot(z, sigmoid(z))
plt.xlabel("z")
plt.ylabel("y")
```

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\sigmoid函数图像案例.png)

这个图像呈现出完美的 S 型（Sigmoid 的含义）。它的取值仅介于 0 和 11之间，且关于 𝑧=0 轴中心对称。同时当 𝑧 越大时，𝑦y 越接近于 1，而 𝑧 越小时，𝑦 越接近于 0。如果我们以 0.5 为分界点，将 >0.5 或 <0.5 的值分为两类。

- 逻辑回归模型

引入一条数学定义。那就是，如果一组连续随机变量符合 **Sigmoid** 函数样本分布，就称作为**逻辑分布**。逻辑分布是概率论中的定理，是一种**连续型的概率分布**。

将二者结合起来，也就是把线性函数拟合的结果**使用 Sigmoid 函数压缩到 (0,1)之间**。如果线性函数的 𝑦y值越大，也就代表概率越接近于 1，反之接近于 0。
$$
逻辑回归中定义：\\
z_{i} = {w_0}{x_0} + {w_1}{x_1} + \cdots + {w_i}{x_i} = {w^T}x \tag{3a}
$$

$$
f(z_{i})=\frac{1}{1+e^{-z_{i}}} \tag{3b}
$$

公式 (3)中，对每一个特征 𝑥x乘上系数 𝑤，然后通过 Sigmoid 函数计算 𝑓(𝑧) 值得到概率。其中，𝑧 可以被看作是分类边界。故：
$$
h_{w}(x) = f({w^T}x)=\frac{1}{1+e^{-w^Tx}}
\tag{4}
$$

$$
由于目标值 y只有 0 和 1 两个值，那么如果记 y=1 的概率为 h_{w}(x)，\\
则此时 y=0 的概率为 1-h_{w}(x)。那么，我们可以记作逻辑回归模型条件概率分布：
$$

$$
P(Y=y | x)=\left\{\begin{array}{rlrl}{h_{w}(x)} & {, y=1} \\ {1-h_{w}(x)} & {, y=0}\end{array}\right.
\tag{5}
$$

公式（5）不方便计算，等价为似然函数：
$$
P(y|x ; w)=\left(h_{w}(x)\right)^{y}\left(1-h_{w}(x)\right)^{1-y}
\tag{6}
$$
以上是1个样本举例，对于i个样本的总概率而言实际上可以看作单样本概率的乘积，记为L(w):
$$
L(w) =\prod_{i=1}^{m}\left(h_{w}\left(x^{(i)}\right)\right)^{y^{(i)}}\left(1-h_{w}\left(x^{(i)}\right)\right)^{1-y^{(i)}}
\tag{7}
$$
使用对数进行转换，即对数似然函数：
$$
\log L(w)=\sum_{i=1}^{m} \left [ y^{(i)} \log h_{w}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{w}\left(x^{(i)}\right)\right)\right ]
\tag{8}
$$

- 对数损失函数

实际上，公式 (8) 被称为对数似然函数，该函数衡量了事件发生的总概率。根据最大似然估计原理，只需要通过对 𝐿(𝑤) 求最大值，即得到 𝑤的估计值。而在机器学习问题中，我们需要一个损失函数，并通过求其最小值来进行参数优化。所以，对数似然函数取负数就可以被作为逻辑回归的对数损失函数：
$$
J(w) =- \frac{1}{m} \sum_{i=1}^{m} \left [ y^{(i)} \log h_{w}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{w}\left(x^{(i)}\right)\right)\right ]
\tag{9}
$$
代码实现

```python
def loss(h, y):
    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    return loss
```

- 梯度下降法

梯度下降法是一种十分常用且经典的**最优化算法**，通过这种方法我们就能**快速找到函数的最小值**。

梯度是一个向量，表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。简而言之，对于一元函数而言，梯度就是指在某一点的导数；而对于多元函数而言，梯度就是指在某一点的偏导数组成的向量。

函数在沿梯度方向变化最快，所以「梯度下降法」的核心就是，我们**沿着梯度下降方向去寻找损失函数的极小值**（**梯度的反方向**）。

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\梯度下降和损失函数.png)

代码实现

```python
def gradient(X, h, y):
    # 梯度计算
    gradient = np.dot((h - y).T, X) / y.shape[0]
    return gradient
```

### 逻辑回归python实现



```python
def sigmoid(z):
    # Sigmoid 分布函数
    sigmoid = 1 / (1 + np.exp(-z))
    return sigmoid


def loss(h, y):
    # 损失函数
    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    return loss


def gradient(X, h, y):
    # 梯度计算
    gradient = np.dot((h - y).T, X) / y.shape[0]
    return gradient

def Logistic_Regression(x, y, lr, num_iter):
    # 逻辑回归过程
    intercept = np.ones((x.shape[0], 1))  # 初始化截距为 1
    x = np.concatenate((intercept, x), axis=1)
    w = np.zeros(x.shape[1])  # 初始化参数为 0

    for i in range(num_iter):  # 梯度下降迭代
        z = np.dot(x, w)  # 线性函数
        h = sigmoid(z)  # sigmoid 函数

        g = gradient(x, h, y)  # 计算梯度
        w -= lr * g  # 通过学习率 lr 计算步长并执行梯度下降

        l = loss(h, y)  # 计算损失函数值

    return l, w  # 返回迭代后的梯度和参数

import pandas as pd

df = pd.read_csv(
    "https://labfile.oss.aliyuncs.com/courses/1081/course-8-data.csv", header=0)  # 加载数据集
df.head()  # 预览前 5 行数据

x = df[['X0', 'X1']].values
y = df['Y'].values
lr = 0.01  # 学习率
num_iter = 30000  # 迭代次数

# 训练
L = Logistic_Regression(x, y, lr, num_iter)
L
```

分类边界线函数：
$$
y=L[1][0]+L[1][1] * x^1+L[1][2] * x^2
$$

- 逻辑回归scikit-learn 实现

```python
LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)

penalty: 惩罚项，默认为  𝐿2  范数。
dual: 对偶化，默认为 False。
tol: 数据解算精度。
fit_intercept: 默认为 True，计算截距项。
random_state: 随机数发生器。
max_iter: 最大迭代次数，默认为 100。
    
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(tol=0.001, max_iter=10000, solver='liblinear')  # 设置数据解算精度和迭代次数
model.fit(x, y)
model.coef_, model.intercept_
```



## 实战

```python
import numpy as np

# 普通最小二乘法代数计算
def ols_algebra(x, y):
    '''
    :param x:
        自变量数组
    :param y:
        因变量数组
    :return:
        w1 —— 线性方程系数
        w0 —— 线性方程截距项
    '''
    n = len(x)
    w1 = (n * sum(x * y) - sum(x) * sum(y)) / (n * sum(x * x) - sum(x) * sum(x))
    w0 = (sum(x * x) * sum(y) - sum(x) * sum(x * y)) / (n * sum(x * x) - sum(x) * sum(x))

    return w1, w0

# def test1():
#     x = np.array([55, 71, 68, 87, 101, 87, 75, 78, 93, 73])
#     y = np.array([91, 101, 87, 109, 129, 98, 95, 101, 104, 93])
#     w1, w0 = ols_algebra(x, y)
#     print(round(w1, 3), round(w0, 3))

# 梯度下降法
def ols_gradient_descent(x, y, lr, num_iter):
    '''
    :param x:
        x -- 自变量数组
    :param y:
        y -- 因变量数组
    :param lr:
        lr -- 学习率
    :param num_iter:
        num_iter -- 迭代次数
    :return:
        w1 -- 线性方程系数
        w0 -- 线性方程截距项
    '''
    n = len(x)
    w1, w0 = 0, 0
    for i in range(num_iter):
        w0 = w0 + lr * 2 * sum(y - w0 - w1 * x)
        w1 = w1 + lr * 2 * sum(x * (y - w0 - w1 * x))
    return w1, w0

def main():
    x = np.array([55, 71, 68, 87, 101, 87, 75, 78, 93, 73])
    y = np.array([91, 101, 87, 109, 129, 98, 95, 101, 104, 93])
    # w1, w0 = ols_algebra(x, y)
    # print('ols_algebra:', round(w1, 3), round(w0, 3))
    w1, w0 = ols_gradient_descent(x, y, lr=0.00001, num_iter=100)
    print('ols_gradient_descent:', round(w1, 3), round(w0, 3))
    return

if __name__ == '__main__':
    main()

```

