[TOC]

# 介绍

AI > 机器学习 > 深度学习。

机器学习是AI的实现手段，深度学习是机器学习的特定方法。

## 机器学习介绍

- 机器学习是AI的一个分支，核心构成为机器学习算法，并通过从数据中获取经验来改善自身性能。

- 机器学习方法：朴素贝叶斯、决策树学习、人工神经网络（ANN，模拟大脑生物结构）等。

- 分类：监督学习、无监督学习、半监督学习、强化学习。

# 线性回归实现与应用

## 介绍

- 线性回归是一种较简单、又重要的机器学习方法。
- 人工神经网络的基石。

## 监督学习

- 通常解决分类和回归问题。
- 训练集数据特征、**标签**，根据测试样本特征预测其样本标签。监督学习中的‘**监督**’体现在训练集具有标签。

### 分类与回归

#### 分类

- 训练集数据特征、**标签**，根据测试样本特征预测其样本标签。监督学习中的‘**监督**’体现在训练集具有标签
- 常见问题类型，对对象进行种类判断。

#### 回归

- 类似分类问题，训练集中数据包含**标签**，也是监督学习特点，但不同于分类问题**预测类别**，回归问题**预测连续实数值**。

- 案例：股票价格预测、房价预测、洪水水位线预测等，——实数值。

## 线性回归

### 介绍

- 以房价预测为例。

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\线性回归-房价预测.png)

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\线性回归-房价预测2.png)

```
首先画了一条红色的直线，让其大致验证橙色点分布的延伸趋势。然后，我将已知房屋的面积大小对应到红色直线上，也就是蓝色点所在位置。最后，再找到蓝色点对应于房屋的价格作为房屋最终的预估价值。
```

在上图呈现的这个过程中，通过找到一条直线去拟合数据点的分布趋势的过程，就是线性回归的过程。而线性回归中的「线性」代指**线性关系**，也就是图中所绘制的**红色直线**。

### 一元线性回归

- 含义：只有1个自变量的线性拟合过程。

- 案例：以上文中房价预测为例

```python
import numpy as np
#  x为房屋面积，单位是平方米; 𝑦 为房价，单位是万元。
x = np.array([56, 72, 69, 88, 102, 86, 76, 79, 94, 74])
y = np.array([92, 102, 86, 110, 130, 99, 96, 102, 105, 92])
# 示例数据由 10 组房屋面积及价格对应组成。接下来，通过 Matplotlib 绘制数据点， 𝑥 ,  𝑦  分别对应着横坐标和纵坐标。
from matplotlib import pyplot as plt
%matplotlib inline

plt.scatter(x, y)
plt.xlabel('Area')
plt.ylabel('Price')
```

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\房价预测案例.png)

```
线性回归即通过线性方程去拟合数据点。那么，我们可以令该 1 次函数的表达式为：
```

$$
y(x, w) = w_0 + w_1x \tag{1}
$$

```python
# 对公式  (1)  进行代码实现：
def f(x, w0, w1):
    y = w0 + w1 * x
    return y
```

想要找出对数据集拟合效果最好的直线，这里再拿出上小节图示进行说明。如下图所示，当我们使用 
$$
 y(x, w) = w_0 + w_1x 
$$
 对数据进行拟合时，就能得到拟合的整体误差，即图中**蓝色线段的长度总和**。如果某一条直线对应的误差值最小，是不是就代表这条直线最能反映数据点的分布趋势呢？

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\房价预测案例-拟合误差.png)

上面的误差往往也称之为**「残差」**。但是在机器学习中，我们更喜欢称作**「损失」**，即真实值和预测值之间的**偏离程度**。那么，对 𝑛 个全部数据点而言，其对应的残差损失总和就为：
$$
\sum\limits_{i = 1}^n {{{(y_{i}-(w_0 + w_1x_{i}))}}} \tag{3}
$$
更进一步，在线性回归中，我们一般使用**残差的平方和**来表示所有更进一步，在线性回归中，我们一般使用残差的平方和来表示所有样本点的误差。公式如下：
$$
\sum\limits_{i = 1}^n {{{(y_{i}-(w_0 + w_1x_{i}))}}^2} \tag{4}
$$

#### 残差平方和

- 损失始终是累加的正数。

- 公式（4），机器学习中专有名词为**「平方损失函数」**。
- 平方损失函数最小，可得到拟合参数w0 和 w1 最优数值。

```python
# 可以对公式  (4)  进行代码实现：
def square_loss(x, y, w0, w1):
    loss = sum(np.square(y - (w0 + w1 * x)))
    return loss
```

#### 最小二乘法及代数求解

最小二乘法是用于**求解线性回归拟合参数 𝑤** 的一种常用方法。最小二乘法中的**「二乘」**代表**平方**，最小二乘也就是**最小平方**。而这里的**平方就是指代上面的平方损失函数**。

推导：

- 平方损失函数为：

$$
f = \sum\limits_{i = 1}^n {{{(y_{i}-(w_0 + w_1x_{i}))}}^2} \tag{5}
$$

- 目标是求取平方损失函数  𝑚𝑖𝑛(𝑓)  最小时，对应的  𝑤 。首先求  𝑓  的 1 阶偏导数：

$$
\frac{\partial f}{\partial w_{0}}=-2(\sum_{i=1}^{n}{y_i}-nw_{0}-w_{1}\sum_{i=1}^{n}{x_i}) \tag{6a}
$$

$$
\frac{\partial f}{\partial w_{1}}=-2(\sum_{i=1}^{n}{x_iy_i}-w_{0}\sum_{i=1}^{n}{x_i}-w_{1}\sum_{i=1}^{n}{x_i}^2) \tag{6b}
$$

- 分别令（6a）、(6b)偏导数 = 0，解得：

$$
w_{1}=\frac {n\sum_{}^{}{x_iy_i}-\sum_{}^{}{x_i}\sum_{}^{}{y_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2} \tag{7b}
$$

$$
w_{0}=\frac {\sum_{}^{}{x_i}^2\sum_{}^{}{y_i}-\sum_{}^{}{x_i}\sum_{}^{}{x_iy_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2} \tag{7b}
$$

#### python实现

```python
def w_calculator(x, y):
    n = len(x)
    w1 = (n * sum(x * y) - sum(x) * sum(y) / (n * sum(x * x) - sum(x) * sum(x)))
    w0 = (sum(x*x)*sum(y) - sum(x)*sum(x*y))/(n*sum(x*x)-sum(x)*sum(x))
    return w0, w1

w0 = w_calculator(x, y)[0]
w1 = w_calculator(x, y)[1]

square_loss(x, y, w0, w1)

x_temp = np.linspace(50, 120, 100)  # 绘制直线生成的临时点

plt.scatter(x, y)
plt.plot(x_temp, x_temp*w1 + w0, 'r')
# 从上图可以看出，拟合的效果还是不错的。那么，如果你手中有一套 150 平米的房产想售卖，获得预估报价就只需要带入方程即可：
f(150, w0, w1)
```

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\房价预测案例-直线生成.png)

#### 机器学习开源模块-sklearn

- 使用 scikit-learn 实现线性回归的过程会简单很多，这里要用到 `LinearRegression()` 类 。看一下其中的参数：

```python
sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)

- fit_intercept: 默认为 True，计算截距项。
- normalize: 默认为 False，不针对数据进行标准化处理。
- copy_X: 默认为 True，即使用数据的副本进行操作，防止影响原数据。
- n_jobs: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。
```

- 实现

```python
from sklearn.linear_model import LinearRegression

# 定义线性回归模型
model = LinearRegression()
# 训练, reshape 操作把数据处理成 fit 能接受的形状
model.fit(x.reshape(len(x), 1), y)  
# 得到模型拟合参数
model.intercept_, model.coef_
# 可以预测 150 平米房产的价格
model.predict([[150]])
```

#### 最小二乘法的矩阵推导及实现

- 一元线性函数的表达式为

$$
y(x, w) = w_0 + w_1x
$$

表达为矩阵形式：
$$
\left[ \begin{array}{c}{1, x_{1}} \\ {1, x_{2}} \\ {\cdots} \\ {1, x_{9}} \\ {1, x_{10}}\end{array}\right] \left[ \begin{array}{c}{w_{0}} \\ {w_{1}}\end{array}\right] = \left[ \begin{array}{c}{y_{1}} \\ {y_{2}} \\ {\cdots} \\ {y_{9}} \\ {y_{10}}\end{array}\right] \Rightarrow \left[ \begin{array}{c}{1,56} \\ {1,72} \\ {\cdots} \\ {1,94} \\ {1,74}\end{array}\right] \left[ \begin{array}{c}{w_{0}} \\ {w_{1}}\end{array}\right]=\left[ \begin{array}{c}{92} \\ {102} \\ {\cdots} \\ {105} \\ {92}\end{array}\right] \tag{8a}
$$
即
$$
y(x, w) = XW \tag{8b}
$$

- 平方损失函数

$$
f = \sum\limits_{i = 1}^n {{{(y_{i}-(w_0 + w_1x_{i}))}}}^2 =(y-XW)^T(y-XW)\tag{9}
$$

通过对公式 (9)实施矩阵计算乘法分配律得到：
$$
f = y^{T}y - y^{T}(XW) - (XW)^{T}y + (XW)^{T}(XW) \tag{10}
$$
在该公式中 𝑦 与 𝑋𝑊 皆为相同形式的 (𝑚,1)矩阵，由此两者相乘属于线性关系，所以等价转换如下：
$$
f = y^{T}y - (XW)^{T}y - (XW)^{T}y + (XW)^{T}(XW)\\ = y^{T}y - 2 (XW)^{T}y + (XW)^{T}(XW) \tag{11}
$$

- 矩阵求偏导数

$$
\frac{\partial f}{\partial W}=2X^TXW-2X^Ty=0 \tag{12}
$$

$$
当矩阵 X^TX 满秩时， (X^TX)^{-1}X^TX=E，且 EW=W。\\
所以有 (X^TX)^{-1}X^TXW=(X^TX)^{-1}X^Ty，并最终得到：
$$

$$
W=(X^TX)^{-1}X^Ty \tag{13}
$$

#### python实现

```python
def w_matrix(x, y):
    w = (x.T * x).I * x.T * y
    return w
# 这里计算时，需要对原  𝑥  数据添加截距项系数 1。
x = np.matrix([[1, 56], [1, 72], [1, 69], [1, 88], [1, 102],
               [1, 86], [1, 76], [1, 79], [1, 94], [1, 74]])
y = np.matrix([92, 102, 86, 110, 130, 99, 96, 102, 105, 92])

w_matrix(x, y.reshape(10, 1))
```



### 线性回归预测实战

```python
import pandas as pd

df = pd.read_csv(
    "D:/事务/我的事务/拓展学习/笔记/data/boston_house_price_dataset.csv")
# 查看 DataFrame 前 5 行数据。
df.head()
'''
该数据集统计了波士顿地区各城镇的住房价格中位数，以及与之相关的特征。每列数据的列名解释如下：
CRIM: 城镇犯罪率。
ZN: 占地面积超过 2.5 万平方英尺的住宅用地比例。
INDUS: 城镇非零售业务地区的比例。
CHAS: 查尔斯河是否经过 (=1 经过，=0 不经过)。
NOX: 一氧化氮浓度（每 1000 万份）。
RM: 住宅平均房间数。
AGE: 所有者年龄。
DIS: 与就业中心的距离。
RAD: 公路可达性指数。
TAX: 物业税率。
PTRATIO: 城镇师生比例。
BLACK: 城镇的黑人指数。
LSTAT: 人口中地位较低人群的百分数。
MEDV: 城镇住房价格中位数。
'''

'''
这里，仅选取 CRIM, RM, LSTAT 三个特征用于线性回归模型训练。我们将这三个特征的数据单独拿出来，并且使用 describe() 方法查看其描述信息。 describe() 统计了每列数据的个数、最大值、最小值、平均数等信息。
'''

'''
同样，我们将目标值单独拿出来。
训练一个机器学习预测模型时，我们通常会将数据集划分为 70% 和 30% 两部分。

其中，70% 的部分被称之为训练集，用于模型训练。
例如，这里的线性回归，就是从训练集中找到最佳拟合参数  𝑤  的值。
另外的 30% 被称为测试集。
对于测试集而言，首先我们知道它对应的真实目标值，然后可以给学习完成的模型输入测试集中的特征，得到预测目标值。最后，通过对比预测的目标值与真实目标值之间的差异，评估模型的预测性能。
'''
'''
接下来，我们针对数据集的特征和目标进行分割，分别得到 70% 的训练集和 30% 的测试集。
其中，训练集特征、训练集目标、测试集特征和测试集目标分别定义为：X_train, y_train, X_test, y_test。
'''
'''
使用了 scikit-learn 模块的 train_test_split 函数完成数据集切分

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test =train_test_split(train_data,train_target,test_size=0.4, random_state=0)

X_train,X_test, y_train, y_test 分别表示，切分后的特征的训练集，特征的测试集，标签的训练集，标签的测试集；其中特征和标签的值是一一对应的。
train_data,train_target分别表示为待划分的特征集和待划分的标签集。
test_size：测试样本所占比例。
random_state：随机数种子,在需要重复实验时，保证在随机数种子一样时能得到一组一样的随机数。
'''
from sklearn.model_selection import train_test_split

target = df['medv']  # 目标值数据

X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.3, random_state=1)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

# 构建和训练模型
# 划分完数据集之后，就可以构建并训练模型。同样，这里要用到 LinearRegression() 类。对于该类的参数就不再重复介绍了。
model = LinearRegression()  # 建立模型
model.fit(X_train, y_train)  # 训练模型
model.coef_, model.intercept_  # 输出训练后的模型参数和截距项
```

$$
f = -0.075 * x_{1} + 4.174 * x_{2} - 0.633 * x_{3} + 4.711 \tag{14}
$$

### 误差分析

#### 平均绝对误差

- 平均绝对误差（MAE）就是绝对误差的平均值，它的计算公式如下：

$$
\textrm{MAE}(y, \hat{y} ) = \frac{1}{n}\sum_{i=1}^{n}{|y_{i}-\hat y_{i}|}\tag{15}\\
其中，y_{i} 表示真实值，\hat y_{i} 表示预测值，n 则表示值的个数。\\
MAE 的值越小，说明模型拥有更好的拟合程度。\\
我们可以尝试使用 Python 实现 MAE 计算函数：
$$

```python
def mse_value(y_true, y_pred):
    n = len(y_true)
    mse = sum(np.square(y_true - y_pred))/n
    return mse
```



#### 均方误差

- 均方误差（MSE）它表示误差的平方的期望值，它的计算公式如下：

$$
\textrm{MSE}(y, \hat{y} ) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^{2}\tag{16}\\
其中，y_{i} 表示真实值，\hat y_{i} 表示预测值，n 则表示值的个数。\\
MSE 的值越小，说明预测模型拥有更好的精确度。\\
同样，我们可以尝试使用 Python 实现 MSE 计算函数：
$$

```python
def mse_value(y_true, y_pred):
    n = len(y_true)
    mse = sum(np.square(y_true - y_pred))/n
    return mse
```



#### 平均绝对百分比误差MAPE

- MAPE 是一个百分比值，因此比其他统计量更容易理解。例如，如果 MAPE 为 5，则表示预测结果较真实结果平均偏离 5%。MAPE 的计算公式如下：

$$
\textrm{MAPE}(y, \hat{y} ) = \frac{\sum_{i=1}^{n}{|\frac{y_{i}-\hat y_{i}}{y_{i}}|}}{n} \times 100 \tag{1}
$$

$$
其中，y_{i} 表示真实值，\hat y_{i} 表示预测值，n 则表示值的个数。\\
MAPE 的值越小，说明预测模型拥有更好的精确度。
$$

```python
def mape(y_true, y_pred):
    n = len(y_true)
    mape = sum(np.abs(1 - y_pred / y_true)) / n * 100
    return mape
```

