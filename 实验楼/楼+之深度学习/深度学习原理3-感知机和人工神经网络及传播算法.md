[TOC]

# 感知机和人工神经网络

## 感知机

> 感知机（英语：Perceptron）是 Frank Rosenblatt 在 1957 年就职于 Cornell 航空实验室时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。

### 感知机的推导

- 二维平面中线性可分

$$
f(x) = w_1x_1+w_2x_2+ \cdots +w_nx_n + b = WX+b \tag{1}
$$

公式（1），可以认为方程是对数据集每一个特征x1,x2,...,xn依次乘上权重w1,w2,...,wn。

- 二分类别

最终类别有2个，通常称为正类别和负类别。当使用线性回归中对应的公式（1）完成分类时，不同于逻辑回归中将f(x)传入simoid函数，现在传入sign函数：
$$
\operatorname{sign}(x)=\left\{\begin{array}{ll}{+1,} & {\text { if } x \geq 0} \\ {-1,} & {\text { if }  x<0}\end{array}\right. \tag{2}
$$
即sign(f(x))，当sign=1，为正分类点，否则=-1，为负分类点。

设输入空间（特征向量）X⊆𝑅^𝑛，输出空间Y=-1，+1。
$$
f(x) = sign(W*x +b) \tag{3}
$$
公式（3）称之为**感知机**。

### 感知机计算流程图

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\感知机计算流程图.png)

### 感知机损失函数

> 在感知机中，会使用误分类点到分割线（面）的距离去定义损失函数。

- 点到直线距离

n维实数向量空间中任意一点x0到直线 **W * x + b = 0**的 距离 为：
$$
d= \dfrac{1}{\parallel W\parallel}|W*x_{0}+b| \tag{4}
$$
其中 ||𝑊||表示 𝐿2范数，即向量各元素的平方和然后开方。
$$
对于点 (x_i,y_i)，使用公式 (3) 进行分类时，如果 W*x_i+b>0，则 sign(W*x_i+b)=1。\\
那么，此点的预测分类为 +1，反之预测分类为 -1。\\
如果此点的真实分类与预测分类不同，则称为误分类点。\\
对于误分类点，真实的 y_i = +1，但使用感知机算出来的 W * x_i + b < 0；\\
或者真实的 y_i = -1，但使用感知机算出来的 W * x_i + b > 0。\\
这两种情况下 y_i(W * x_{i}+ b) < 0 均成立，即对于误分类点，公式 (5) 成立。
$$
点面距 或 误分类点到分割线：
$$
d=-\dfrac{1}{\parallel W\parallel}y_i(W*x_{i}+b) \tag{6}
$$
感知机损失函数：
$$
J(W,b) = - \sum_{x_i\epsilon M} y_i(W*x_{i}+b) \tag{8}
$$
从公式 (8) 可以看出，**损失函数 𝐽(𝑊,𝑏) 是非负**的。也就是说，当没有误分类点时，损失函数的值为 **0**。同时，**误分类点越少**，误分类点距离分割线（面）就**越近**，损失函数值就**越小**。同时，损失函数 𝐽(𝑊,𝑏)J(W,b) 是**连续可导函数**。

### 随机梯度下降法

> 类似于逻辑回归中，为了找到损失函数的极小值，也采用了一种梯度下降法的改进方法，也称为随机梯度下降法SGD。

- SGD随机梯度下降算法实现

```python
from sklearn.utils import shuffle


def perceptron_sgd(X, Y, alpha, epochs):
    """
    参数:
    X -- 自变量数据矩阵
    Y -- 因变量数据矩阵
    alpha -- lamda 参数
    epochs -- 迭代次数

    返回:
    w -- 权重系数
    b -- 截距项
    """
    # 感知机随机梯度下降算法实现
    w = np.zeros(len(X[0]))  # 初始化参数为 0
    b = np.zeros(1)

    for t in range(epochs):  # 迭代
        # 每一次迭代循环打乱训练样本
        # X, Y = shuffle(X, Y)
        for i, x in enumerate(X):
            if ((np.dot(X[i], w)+b)*Y[i]) <= 0:  # 判断条件
                w = w + alpha*X[i]*Y[i]  # 更新参数
                b = b + alpha*Y[i]

    return w, b
```

### 感知机分类实例

```python
# 数据集
import pandas as pd

df = pd.read_csv(
    "https://labfile.oss.aliyuncs.com/courses/1081/course-12-data.csv", header=0)  # 加载数据集
df.head()  # 预览前 5 行数据

'''
该数据集共有两个特征变量 X0 和 X1, 以及一个目标值 Y。其中，目标值 Y 只包含 -1 和 1。
'''

# 使用感知机求解最佳分割线。
import numpy as np

X = df[['X0', 'X1']].values
Y = df['Y'].values

alpha = 0.1
epochs = 150

perceptron_sgd(X, Y, alpha, epochs)
```

求得的最佳分割线方程：
$$
f(x)=4.93 * x_{1}-6.98 * x_{2}-3.3 \tag{12}
$$


```python
def perceptron_loss(X, Y, alpha, epochs):
    """
    参数:
    X -- 自变量数据矩阵
    Y -- 因变量数据矩阵
    alpha -- lamda 参数
    epochs -- 迭代次数

    返回:
    loss_list -- 每次迭代损失函数值列表
    """
    # 计算每次迭代后的损失函数值
    w = np.zeros(len(X[0]))  # 初始化参数为 0
    b = np.zeros(1)
    loss_list = []

    for t in range(epochs):  # 迭代
        loss_init = 0
        for i, x in enumerate(X):
            # 每一次迭代循环打乱训练样本
            # X, Y = shuffle(X, Y)
            if ((np.dot(X[i], w)+b)*Y[i]) <= 0:  # 判断条件
                loss_init += (((np.dot(X[i], w)+b)*Y[i]))
                w = w + alpha*X[i]*Y[i]  # 更新参数
                b = b + alpha*Y[i]
        loss_list.append(loss_init * -1)

    return loss_list
```

当数据集线性可分，却造成损失函数变换曲线震荡的原因：

1. **学习率太大**。
2. **迭代次数太少**。

```python
alpha = 0.05  # 减小学习率
epochs = 1000  # 增加迭代次数
```

## 人工神经网络

> 感知机只能处理二分类问题，且必须是线性可分问题。
>
> 人工神经网络，即 Artificial Neural Network（ANN）

### 多层感知机与人工神经网络

- 人工神经网络

感知机也是1个人工神经网络，是简单的**单层神经网络**。

人工神经网络可以用来解决 **线性不可分** 或者 **多分类问题**， 将**多个感知机组合**。

人工神经网络某种意义上 代指 **多层感知机**。

- 单层感知机的精简流程图

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\感知机计算流程图-精简.png)

如上只包含1个输入层的网络结构 可以 称之为 单层神经网络结构。

- 多层感知机

> 将1个感知机的输出作为另一个感知机的输入。

输入层和输出层之间的称为**隐含层**。

一个神经网络结构计算层数时，一般**只计算输入和隐含层的数量**

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\神经网络结构图-1个隐含层.png)

上图中为1个**2层神经网络结构**。

### 激活函数

$$
f(x) = w_1x_1+w_2x_2+ \cdots +w_nx_n + b = WX+b \tag{13}
$$

- 逻辑回归

采用sigmoid函数，将f(x)转为 概率， 最终实现 二分类。

- 感知机

采用sign函数， 将f(x)转为-1 和 1 ，最终实现二分类。

- 多层感知机

具有多层感知机。

> 上述中sigmoid、sign也称 激活函数。

#### 激活函数作用

- 简而言之

针对数据进行**非线性变换**。

因为**线性变换的多重组合依旧是线性变换**，没有意义，而加入激活函数，即引入了**非线性因**素，可以解决线性模型无法完成的分类任务。

### 反向传播算法BP

> 之前感知机中，定义了损失函数，通过随机梯度下降方法求解最优参数。
>
> 而感知机只有一层，求解梯度较简单，在多层结构中，更新权重过程变得复杂，而反向传播算法BP可帮助快速求解梯度。

- 3层神经网络结构

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\3层神经网络结构图.png)

1. 有2个输入x1 、 x2  和 1个输出 y。

2. 每个紫色单元表示1个独立神经元，分别由2个单元组成，1个单元是权重和输入信号，1个是激活函数，其中，e表示激活信号，所以 y=f(e)表示被激活函数处理之后的非线性输出，即神经元的输出。

   ![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\神经元处理流图.png)

3. 

#### 前向传播过程

> 开始训练神经网络，训练数据由输入信号x1 和 x2 以及 期望输出z组成。

- 计算第1个隐含层中第1个神经元y1 = f1(e)对应值

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\前向传播1.png)

- 计算第1个隐含层中第3个神经元y2 = f2(e)值

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\前向传播2.png)

- ...
- 第2个隐含层第1个神经元

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\前向传播4.png)

- ...
- 得到输出层结果

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\前向传播6.png)

#### 反向传播

> 得到前向传播的输出结果y时， 和期望输出z 对比， 得到误差𝛿，然后根据误差𝛿，沿着神经元回路反向传递，每个神经元对应误差即为传递的误差 * 权重。

- 计算第2层隐含层第1个神经元的误差计算

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\反向传播-误差1.png)

- ...
- 计算第1层隐含层第1个神经元误差

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\反向传播-误差3.png)

- ...

> 利用反向传递的误差，从输入层开始，依次更新权重w。

- 更新权重w11

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\反向传播-权重更新1.png)

- ...
- 更新权重w46 和 w56

![](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\反向传播-权重更新6.png)

>  𝜂 表示 学习速率

以上：前向传播计算神经元值 - 获取误差 - 反向逐层计算误差 - 前向更新权重。为1个迭代过程。



### python实现人工神经网络

> 使用简易的2层人工神经网络结构，1层隐含层（3个神经元），1层输入层（2个神经元），通过输出层实现2分类问题求解

- 人工神经网络结构图

![python实现的人工神经网络](D:\事务\我的事务\拓展学习\笔记\pictures\楼+深度学习\前向传播和反向传播\python实现的人工神经网络.png)

- 激活函数sigmoid函数

$$
\mathit{sigmoid}(x) = \frac{1}{1+e^{-x}}       \tag{17a}
$$

其导数公式：
$$
\Delta \mathit{sigmoid}(x)  = \mathit{sigmoid}(x)(1 - \mathit{sigmoid}(x))    \tag{17b}
$$
python实现

```python
def sigmoid(x):
    # sigmoid 函数
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    # sigmoid 函数求导
    return sigmoid(x) * (1 - sigmoid(x))
```

- 前向传播

前向传播中，每一个神经元计算流程：**线性变换 - 激活函数 - 输出值**。

> - 𝑍 表示隐含层输出，𝑌 则为输出层最终输出。
> - 𝑤𝑖𝑗 表示从第 𝑖 层的第 𝑗 个权重。

所以有输入X，第一层权重 W1 ，第二层权重 W2 ，4.
$$
X=\left[ \begin{array}{ll}{x_{1}} & {x_{2}}\end{array}\right] \tag{18}
$$

$$
W_{1}=\left[ \begin{array}{lll}{w_{11}} & {w_{12}} & {w_{13}} \\ {w_{14}} & {w_{15}} & {w_{16}}\end{array}\right] \tag{19}
$$

$$
W_{2}=\left[ \begin{array}{c}{w_{21}} \\ {w_{22}} \\ {w_{23}}\end{array}\right] \tag{20}
$$

为方便计算清晰，假设此处截距项 = 0，则隐含层神经元输出Z 。
$$
Z = \mathit{sigmoid}(X \cdot W_{1}) \tag{21}
$$
输出层Y：
$$
Y = \mathit{sigmoid}(Z \cdot W_{2}) \tag{22}
$$
python

```python
# 示例样本
X = np.array([[1, 1]])
y = np.array([[1]])

# 然后，随机初始化隐含层权重。
W1 = np.random.rand(2, 3) # 2行3列 的随机数组
W2 = np.random.rand(3, 1)

# 前向传播的过程实现基于公式  (21)  和公式  (22)  完成
input_layer = X  # 输入层
hidden_layer = sigmoid(np.dot(input_layer, W1))  # 隐含层，公式 20
output_layer = sigmoid(np.dot(hidden_layer, W2))  # 输出层，公式 22
```



- 反向传播

> - 使用梯度下降法 优化神经网络的 参数。
>
> - 定义损失函数
> - 计算损失函数关于神经网络中各层的权重的偏导数（梯度）。

设神经网络的输出值Y ， 真实值 y。

定义平方损失函数：
$$
Loss(y, Y) = \sum (y - Y)^2 \tag{23}
$$
求解梯度（链式求导法则）：
$$
\frac{\partial Loss(y, Y)}{\partial{W_2}} = \frac{\partial Loss(y, Y)}{\partial{Y}} \frac{\partial Y}{\partial{W_2}}\tag{24a}
$$

$$
\frac{\partial Loss(y, Y)}{\partial{W_2}} = 2(Y-y) * \Delta \mathit{sigmoid}(Z \cdot W_2) \cdot Z\tag{24b}
$$

同理对W1 求梯度
$$
\frac{\partial Loss(y, Y)}{\partial{W_1}} = \frac{\partial Loss(y, Y)}{\partial{Y}} \frac{\partial Y }{\partial{Z}} \frac{\partial Z}{\partial{W_1}} \tag{25a}
$$

$$
\frac{\partial Loss(y, Y)}{\partial{W_1}} = 2(Y-y) * \Delta \mathit{sigmoid}(Z \cdot W_2) \cdot W_2 * \Delta \mathit{sigmoid}(X \cdot W_1) \cdot X \tag{25b}
$$

python

```python
# 公式 24
d_W2 = np.dot(hidden_layer.T, (2 * (output_layer - y) *
                               sigmoid_derivative(np.dot(hidden_layer, W2))))

# 公式 25
d_W1 = np.dot(input_layer.T, (
    np.dot(2 * (output_layer - y) * sigmoid_derivative(
           np.dot(hidden_layer, W2)), W2.T) * sigmoid_derivative(np.dot(input_layer, W1))))

# 设置学习率，对W1 W2进行一次更新
# 梯度下降更新权重, 学习率为 0.05
W1 -= 0.05 * d_W1  # 如果上面是 y - output_layer，则改成 +=
W2 -= 0.05 * d_W2
```

> 以上，我们就实现了单个样本在神经网络中的 1 次前向 → 反向传递，并使用梯度下降完成 1 次权重更新。

完整神经网络

```python
# 示例神经网络完整实现
class NeuralNetwork:

    # 初始化参数
    def __init__(self, X, y, lr):
        self.input_layer = X
        self.W1 = np.random.rand(self.input_layer.shape[1], 3)
        self.W2 = np.random.rand(3, 1)
        self.y = y
        self.lr = lr
        self.output_layer = np.zeros(self.y.shape)

    # 前向传播
    def forward(self):
        self.hidden_layer = sigmoid(np.dot(self.input_layer, self.W1))
        self.output_layer = sigmoid(np.dot(self.hidden_layer, self.W2))

    # 反向传播
    def backward(self):
        d_W2 = np.dot(self.hidden_layer.T, (2 * (self.output_layer - self.y) *                       sigmoid_derivative(np.dot(self.hidden_layer, self.W2))))

        d_W1 = np.dot(self.input_layer.T, (
            np.dot(2 * (self.output_layer - self.y) * sigmoid_derivative(
                   np.dot(self.hidden_layer, self.W2)), self.W2.T) * sigmoid_derivative(np.dot(self.input_layer, self.W1))))

        # 参数更新
        self.W1 -= self.lr * d_W1
        self.W2 -= self.lr * d_W2
        
# 开始测试
X = df[['X0', 'X1']].values  # 输入值
y = df[['Y']].values # 真实 y

# 将其输入到网络中，并迭代 100 次
nn = NeuralNetwork(X, y, lr=0.001)  # 定义模型
loss_list = []  # 存放损失数值变化

for i in range(100):
    nn.forward()  # 前向传播
    nn.backward()  # 反向传播
    loss = np.sum((y - nn.output_layer)**2)  # 计算平方损失
    loss_list.append(loss)

print("final loss:", loss)
plt.plot(loss_list)  # 绘制 loss 曲线变化图
```

由于权重是**随机初始化**，多次运行的**结果会不同**。