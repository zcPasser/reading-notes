[TOC]

# TensorFlowæ„å»ºç¥ç»ç½‘ç»œ

## Numpyæ„å»ºç¥ç»ç½‘ç»œ

- ä½¿ç”¨ **Numpy** æ„å»º **å…¨è¿æ¥ç¥ç»ç½‘ç»œ** 

> **å…¨è¿æ¥** æŒ‡ å…¶æ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½ä¸ä¸Šä¸€å±‚æ¯ä¸ªèŠ‚ç‚¹ç›¸è¿ã€‚

- ç¥ç»ç½‘ç»œä»£ç å®ç°

```python
'''
	å¯¹äºç¥ç»ç½‘ç»œçš„å®ç°ï¼Œä¸»è¦æ˜¯å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸¤ä¸ªéƒ¨åˆ†ã€‚å‰å‘ä¼ æ’­å½“ç„¶æ˜¯ä»è¾“å…¥ â†’ è¾“å‡ºçš„è®¡ç®—ï¼Œè€Œåå‘ä¼ æ’­åˆ™é€šè¿‡è®¡ç®—æ¢¯åº¦æ¥æ›´æ–°ç½‘ç»œæƒé‡ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ç›´æ¥å°†å‰é¢æ„ŸçŸ¥æœºå’Œäººå·¥ç¥ç»ç½‘ç»œå®éªŒä¸­å†™è¿‡çš„ä»£ç æ‹¿è¿‡æ¥ä½¿ç”¨ã€‚
'''
import numpy as np

class NeuralNetwork:
    def __init__(self, X, y, lr):
        """åˆå§‹åŒ–å‚æ•°"""
        self.input_layer = X
        self.W1 = np.ones((self.input_layer.shape[1], 3))  # åˆå§‹åŒ–æƒé‡å…¨ä¸º 1
        self.W2 = np.ones((3, 1))
        self.y = y
        self.lr = lr

    def forward(self):
        """å‰å‘ä¼ æ’­"""
        self.hidden_layer = sigmoid(np.dot(self.input_layer, self.W1))
        self.output_layer = sigmoid(np.dot(self.hidden_layer, self.W2))
        return self.output_layer

    def backward(self):
        """åå‘ä¼ æ’­"""
        d_W2 = np.dot(self.hidden_layer.T, (2 * (self.output_layer - self.y) *
                                            sigmoid_derivative(np.dot(self.hidden_layer, self.W2))))

        d_W1 = np.dot(self.input_layer.T, (
            np.dot(2 * (self.output_layer - self.y) * sigmoid_derivative(
                   np.dot(self.hidden_layer, self.W2)), self.W2.T) * sigmoid_derivative(
                np.dot(self.input_layer, self.W1))))

        # å‚æ•°æ›´æ–°
        self.W1 -= self.lr * d_W1
        self.W2 -= self.lr * d_W2
        
# ä½¿ç”¨ç¤ºä¾‹æ•°æ®å®Œæˆç¥ç»ç½‘ç»œè®­ç»ƒã€‚
import pandas as pd

# ç›´æ¥è¿è¡ŒåŠ è½½æ•°æ®é›†
df = pd.read_csv(
    "https://labfile.oss.aliyuncs.com/courses/1081/course-12-data.csv", header=0)

from matplotlib import pyplot as plt
%matplotlib inline

X = df[['X0', 'X1']].values  # è¾“å…¥å€¼
y = df[['Y']].values  # çœŸå® y
nn_model = NeuralNetwork(X, y, lr=0.001)  # å®šä¹‰æ¨¡å‹
loss_list = []  # å­˜æ”¾æŸå¤±æ•°å€¼å˜åŒ–

def sigmoid(x):
    """æ¿€æ´»å‡½æ•°"""
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    """sigmoid å‡½æ•°æ±‚å¯¼"""
    return sigmoid(x) * (1 - sigmoid(x))

# è¿­ä»£ 200 æ¬¡
for _ in range(200):
    y_ = nn_model.forward()  # å‰å‘ä¼ æ’­
    nn_model.backward()  # åå‘ä¼ æ’­
    loss = np.square(np.subtract(y, y_)).mean()  # è®¡ç®— MSE æŸå¤±
    loss_list.append(loss)

plt.plot(loss_list)  # ç»˜åˆ¶ loss æ›²çº¿å˜åŒ–å›¾
plt.title(f"final loss: {loss}")
```

## TensorFlowæ„å»ºç¥ç»ç½‘ç»œ

### æ­¥éª¤

- å¤„ç†å¼ é‡æ•°æ®ã€‚
- å®šä¹‰æ¨¡å‹ç±»ã€‚
- MSEæŸå¤±å‡½æ•°ã€‚
- æ¢¯åº¦ä¸‹é™ä¼˜åŒ–è¿­ä»£ã€‚
- ä½¿ç”¨TensorFlowä¼˜åŒ–å™¨ã€‚

### ä»£ç 

```python
# é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®Œæˆå¯¹è¾“å…¥æ•°æ®ç‰¹å¾å’Œç›®æ ‡å€¼çš„è½¬æ¢ï¼Œå°†å…¶å…¨éƒ¨è½¬æ¢ä¸ºå¼ é‡ã€‚
import tensorflow as tf

# å°†æ•°ç»„è½¬æ¢ä¸ºå¸¸é‡å¼ é‡
X = tf.cast(tf.constant(df[['X0', 'X1']].values), tf.float32)
y = tf.constant(df[['Y']].values)
'''
tf.cast ä¸»è¦ç”¨äºè½¬æ¢å¼ é‡ç±»å‹ä¸º tf.float32ï¼Œè¿™æ˜¯ä¸ºäº†å’Œåé¢æƒé‡å¼ é‡ç±»å‹ç»Ÿä¸€ã€‚é€šè¿‡è¾“å‡ºå¯ä»¥çœ‹å‡ºï¼Œæ ·æœ¬ä¸º 150 ä¸ªï¼Œç‰¹å¾ä¸º 2 ä¸ªï¼Œç›®æ ‡å€¼ 1 ä¸ªã€‚
'''

# æ„å»ºå‰å‘ä¼ æ’­è®¡ç®—å›¾
'''
è¿™éƒ¨åˆ†ä¸ NumPy æ„å»ºå‰å‘ä¼ æ’­è¿‡ç¨‹éå¸¸ç›¸ä¼¼ï¼Œåªæ˜¯æ›´æ¢ä¸ºä½¿ç”¨ TensorFlow æ„å»ºã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šå°†å‰å‘ä¼ æ’­è¿‡ç¨‹ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ç±»å°è£…ï¼Œå¹¶ä½¿ç”¨ TensorFlow æä¾›çš„ tf.Variable éšæœºåˆå§‹åŒ–å‚æ•°  ğ‘Š 
'''
class Model(object):
    def __init__(self):
        # åˆå§‹åŒ–æƒé‡å…¨ä¸º 1ï¼Œä¹Ÿå¯ä»¥éšæœºåˆå§‹åŒ–
        # é€‰æ‹©å˜é‡å¼ é‡ï¼Œå› ä¸ºæƒé‡åç»­ä¼šä¸æ–­è¿­ä»£æ›´æ–°
        self.W1 = tf.Variable(tf.ones([2, 3]))
        self.W2 = tf.Variable(tf.ones([3, 1]))

    def __call__(self, x):
        hidden_layer = tf.nn.sigmoid(tf.linalg.matmul(X, self.W1))  # éšå«å±‚å‰å‘ä¼ æ’­
        y_ = tf.nn.sigmoid(tf.linalg.matmul(hidden_layer, self.W2))  # è¾“å‡ºå±‚å‰å‘ä¼ æ’­
        return y_
    
# å®ä¾‹åŒ–æ¨¡å‹ç±»ï¼Œå¹¶ä¼ å…¥è¾“å…¥æ•°ç»„è¿›è¡Œç®€å•æµ‹è¯•ã€‚
model = Model()  # å®ä¾‹åŒ–ç±»
y_ = model(X)  # æµ‹è¯•è¾“å…¥
y_.shape  # è¾“å‡º

'''
ä¸Šæ–¹æ„å»ºç½‘ç»œçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è°ƒç”¨äº† tf.nn æ¨¡å— ä¸‹çš„ sigmoid æ¿€æ´»å‡½æ•°ã€‚tf.nn æ˜¯ TensorFlow æ„å»ºç¥ç»ç½‘ç»œå¸¸ç”¨çš„æ¨¡å—ï¼Œå…¶åŒ…å«å°è£…å¥½çš„ç¥ç»ç½‘ç»œå±‚ï¼Œæ¿€æ´»å‡½æ•°ï¼Œå°‘é‡çš„æŸå¤±å‡½æ•°æˆ–å…¶ä»–é«˜é˜¶ API ç»„ä»¶ã€‚
'''

'''
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰è®­ç»ƒæ‰€éœ€çš„æŸå¤±å‡½æ•°ã€‚æŸå¤±å‡½æ•°å’Œ NumPy æ„å»ºæ—¶ä¸€æ ·ï¼Œè¿™é‡Œé€‰æ‹©å¹³æ–¹å’ŒæŸå¤±å‡½æ•°ã€‚å…¶åŒ…å«åœ¨ tf.losses æ¨¡å—  ä¸‹æ–¹ã€‚è¯¥æ¨¡å—åŒ…å«ä¸€äº›æ¯”è¾ƒåŸºç¡€çš„æŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚è¿™é‡Œç”¨åˆ°çš„ MSEã€‚
ä¸ºäº†æ›´æ–¹ä¾¿åç»­è°ƒç”¨ï¼Œæˆ‘ä»¬è¿™é‡Œéœ€è¦å°† tf.losses.mean_squared_error MSE æŸå¤±å‡½æ•°è®¡ç®—æ–¹æ³•å°è£…æˆä¸€ä¸ªæ›´å®Œå–„çš„æŸå¤±å‡½æ•°ã€‚ç‰¹åˆ«åœ°ï¼Œå°†å„æ ·æœ¬æŸå¤±ä½¿ç”¨ tf.reduce_mean æ–¹æ³•æ±‚å’Œï¼Œå¾—åˆ°æ ·æœ¬æ€»æŸå¤±ã€‚
'''
def loss_fn(model, X, y):
    y_ = model(X)  # å‰å‘ä¼ æ’­å¾—åˆ°é¢„æµ‹å€¼
    # ä½¿ç”¨ MSE æŸå¤±å‡½æ•°ï¼Œå¹¶ä½¿ç”¨ reduce_mean è®¡ç®—æ ·æœ¬æ€»æŸå¤±
    loss = tf.reduce_mean(tf.losses.mean_squared_error(y_true=y, y_pred=y_))
    return loss
# ç®€å•æµ‹è¯•æŸå¤±å‡½æ•°æ˜¯å¦æ‰§è¡Œæ­£å¸¸
loss = loss_fn(model, X, y)
loss

'''
å®šä¹‰å®ŒæŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥å®Œæˆæ¨¡å‹å‚æ•°çš„è¿­ä»£ä¼˜åŒ–äº†ã€‚å‰é¢å·²ç»å­¦ä¹ è¿‡äº†ï¼ŒTensorFlow 2 ä¸­çš„ Eager Execution æä¾›äº† tf.GradientTape ç”¨äºè¿½è¸ªæ¢¯åº¦ï¼Œç„¶åä½¿ç”¨ tape.gradient æ–¹æ³•å°±å¯ä»¥è®¡ç®—æ¢¯åº¦äº†ã€‚
'''
EPOCHS = 200  # è¿­ä»£ 200 æ¬¡
LEARNING_RATE = 0.1  # å­¦ä¹ ç‡

for epoch in range(EPOCHS):
    # ä½¿ç”¨ GradientTape è¿½è¸ªæ¢¯åº¦
    with tf.GradientTape() as tape:
        loss = loss_fn(model, X, y)  # è®¡ç®— Lossï¼ŒåŒ…å«å‰å‘ä¼ æ’­è¿‡ç¨‹
    # ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–è¿­ä»£
    # è¾“å‡ºæ¨¡å‹éœ€ä¼˜åŒ–å‚æ•° W1ï¼ŒW2 è‡ªåŠ¨å¾®åˆ†ç»“æœ
    dW1, dW2 = tape.gradient(loss, [model.W1, model.W2])
    model.W1.assign_sub(LEARNING_RATE * dW1)  # æ›´æ–°æ¢¯åº¦
    model.W2.assign_sub(LEARNING_RATE * dW2)

    # æ¯ 100 ä¸ª Epoch è¾“å‡ºå„é¡¹æŒ‡æ ‡
    if epoch == 0:
        print(f'Epoch [000/{EPOCHS}], Loss: [{loss:.4f}]')
    elif (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: [{loss:.4f}]')
'''
å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œtape.gradient() ç¬¬äºŒä¸ªå‚æ•°æ”¯æŒä»¥åˆ—è¡¨å½¢å¼ä¼ å…¥å¤šä¸ªå‚æ•°åŒæ—¶è®¡ç®—æ¢¯åº¦ã€‚ç´§æ¥ç€ï¼Œä½¿ç”¨ .assign_sub å³å¯å®Œæˆå…¬å¼ä¸­çš„å‡æ³•æ“ä½œç”¨ä»¥æ›´æ–°æ¢¯åº¦ã€‚ä½ å¯ä»¥çœ‹åˆ°ï¼ŒæŸå¤±å‡½æ•°çš„å€¼éšç€è¿­ä»£è¿‡ç¨‹ä¸æ–­å‡å°ï¼Œæ„å‘³ç€æˆ‘ä»¬ç¦»æœ€ä¼˜åŒ–å‚æ•°ä¸æ–­æ¥è¿‘ã€‚
'''

'''
ä¸Šé¢ï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ„é€ äº†ä¸€ä¸ªæ¢¯åº¦ä¸‹é™è¿­ä»£è¿‡ç¨‹ã€‚å®é™…åº”ç”¨ä¸­å¹¶ä¸ç»å¸¸è¿™æ ·åšï¼Œè€Œæ˜¯ä½¿ç”¨ TensorFlow æä¾›çš„ç°æˆä¼˜åŒ–å™¨ã€‚ä½ å¯ä»¥æŠŠä¼˜åŒ–å™¨ç†è§£ä¸ºå¯¹è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹çš„é«˜é˜¶å°è£…ï¼Œæ–¹ä¾¿æˆ‘ä»¬æ›´å¿«é€Ÿå®Œæˆæ¨¡å‹è¿­ä»£è¿‡ç¨‹ã€‚
ç”±äºéšæœºæ¢¯åº¦ä¸‹é™è¿œæ¯”æ™®é€šæ¢¯åº¦ä¸‹é™å¸¸ç”¨ï¼Œæ‰€ä»¥ TensorFlow æ²¡æœ‰æä¾›æ™®é€šæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬é€‰æ‹©éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨å¯¹å‚æ•°è¿›è¡Œæ›´æ–°ã€‚ä¼˜åŒ–å™¨ä¸€èˆ¬æ”¾åœ¨ tf.optimizers æ¨¡å—  ä¸‹æ–¹ã€‚
'''
# å®šä¹‰ SGD ä¼˜åŒ–å™¨ï¼Œè®¾å®šå­¦ä¹ ç‡ï¼Œ
optimizer = tf.optimizers.SGD(learning_rate=0.1)
optimizer
# ä½¿ç”¨ä¼˜åŒ–å™¨æ›¿ä»£ä¸Šæ–¹çš„æ‰‹åŠ¨æ„å»ºæ¢¯åº¦ä¸‹é™è¿‡ç¨‹ã€‚
loss_list = []  # å­˜æ”¾æ¯ä¸€æ¬¡ loss
model = Model()  # å®ä¾‹åŒ–ç±»
for epoch in range(EPOCHS):
    # ä½¿ç”¨ GradientTape è¿½è¸ªæ¢¯åº¦
    with tf.GradientTape() as tape:
        loss = loss_fn(model, X, y)  # è®¡ç®— Lossï¼ŒåŒ…å«å‰å‘ä¼ æ’­è¿‡ç¨‹
        loss_list.append(loss)  # ä¿å­˜æ¯æ¬¡è¿­ä»£ loss

    grads = tape.gradient(loss, [model.W1, model.W2])  # è¾“å‡ºè‡ªåŠ¨å¾®åˆ†ç»“æœ
    optimizer.apply_gradients(zip(grads, [model.W1, model.W2]))  # ä½¿ç”¨ä¼˜åŒ–å™¨æ›´æ–°æ¢¯åº¦

    # æ¯ 100 ä¸ª Epoch è¾“å‡ºå„é¡¹æŒ‡æ ‡
    if epoch == 0:
        print(f'Epoch [000/{EPOCHS}], Loss: [{loss:.4f}]')
    elif (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: [{loss:.4f}]')

plt.plot(loss_list)  # ç»˜åˆ¶ loss å˜åŒ–å›¾åƒ
```

ä½¿ç”¨tensorflowæ„å»ºç®€å•ç¥ç»ç½‘ç»œå…¨éƒ¨ä»£ç 

```python
class Model(object):
    def __init__(self):
        self.W1 = tf.Variable(tf.ones([2, 3]))
        self.W2 = tf.Variable(tf.ones([3, 1]))

    def __call__(self, x):
        hidden_layer = tf.nn.sigmoid(tf.linalg.matmul(X, self.W1))
        y_ = tf.nn.sigmoid(tf.linalg.matmul(hidden_layer, self.W2))
        return y_

def loss_fn(model, X, y):
    y_ = model(X)
    loss = tf.reduce_mean(tf.losses.mean_squared_error(y_true=y, y_pred=y_))
    return loss

X = tf.cast(tf.constant(df[['X0', 'X1']].values), tf.float32)
y = tf.constant(df[['Y']].values)

model = Model()
EPOCHS = 200

for epoch in range(EPOCHS):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, X, y)
    grads = tape.gradient(loss, [model.W1, model.W2])
    optimizer = tf.optimizers.SGD(learning_rate=0.1)
    optimizer.apply_gradients(zip(grads, [model.W1, model.W2]))
```

## Numpyå’ŒTensorFlowæ„å»ºç¥ç»ç½‘ç»œæ€»ç»“

- Numpy

æ„å»ºç¥ç»ç½‘ç»œï¼šå®šä¹‰æ•°æ® â†’ å‰å‘ä¼ æ’­ â†’ æ‰‹åŠ¨æ¨å¯¼æ¢¯åº¦è®¡ç®—å…¬å¼ â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°æƒé‡ â†’ è¿­ä»£ä¼˜åŒ–ã€‚

- TensorFlow

æ„å»ºç¥ç»ç½‘ç»œï¼šå®šä¹‰å¼ é‡ â†’ å®šä¹‰å‰å‘ä¼ æ’­æ¨¡å‹ â†’ å®šä¹‰æŸå¤±å‡½æ•° â†’ å®šä¹‰ä¼˜åŒ–å™¨ â†’ è¿­ä»£ä¼˜åŒ–ã€‚

## DIGITSåˆ†ç±»

- åŠ è½½æ•°æ®é›†

```python
from sklearn.datasets import load_digits

digits = load_digits()  # è¯»å–æ•°æ®

digits_X = digits.data  # ç‰¹å¾å€¼
digits_y = digits.target  # æ ‡ç­¾å€¼

digits_X.shape, digits_y.shape
```

- æ•°æ®é¢„å¤„ç†

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†ç›®æ ‡å€¼å¤„ç†æˆ**ç‹¬çƒ­ç¼–ç **çš„å½¢å¼ã€‚ç‹¬çƒ­ç¼–ç åœ¨å…ˆå‰çš„å†…å®¹ä¸­æœ‰è¿‡ä»‹ç»ï¼Œæ•°æ®å¯¹åº”çš„ç›®æ ‡æ˜¯**æ•°å­— 0 ï½ 9**ï¼Œå¤„ç†æˆç‹¬çƒ­ç¼–ç ä¸ºï¼š 

|    0 |    â†’ |  1   |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |
| ---: | ---: | :--: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
|    1 |    â†’ |  0   |    1 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |
|    2 |    â†’ |  0   |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |
|    3 |    â†’ |  0   |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |
|    4 |    â†’ |  0   |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |
|    5 |    â†’ |  0   |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |
|    6 |    â†’ |  0   |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |
|    7 |    â†’ |  0   |    0 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |
|    8 |    â†’ |  0   |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |
|    9 |    â†’ |  0   |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    1 |

å¤„ç†æˆç‹¬çƒ­ç¼–ç çš„åŸå› ä¼šåœ¨åé¢è¿›è¡Œè¯´æ˜ã€‚ä½¿ç”¨ NumPy è¿›è¡Œç‹¬çƒ­ç¼–ç è½¬æ¢å¯ä»¥å€ŸåŠ© `np.eye` ç”Ÿæˆ**å¯¹è§’çŸ©é˜µ**ï¼Œç„¶ååœ¨**å¯¹åº”ä½ç½®å¡«å…… 1** æ¥å®Œæˆï¼Œè¿™æ˜¯ä¸€ä¸ªå¤„ç†çš„å°æŠ€å·§ã€‚



```python
digits_y = np.eye(10)[digits_y.reshape(-1)]

# æ•°æ®åˆ‡åˆ†ã€‚åˆ†ä¸º 80% è®­ç»ƒé›†å’Œ 20% æµ‹è¯•é›†
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    digits_X, digits_y, test_size=0.2, random_state=1)

X_train.shape, X_test.shape, y_train.shape, y_test.shape
```

- å®šä¹‰æ¨¡å‹ç±»

æ„å»º3å±‚ç¥ç»ç½‘ç»œï¼Œå…¶ä¸­2å±‚éšå«å±‚ã€‚

![](D:\äº‹åŠ¡\æˆ‘çš„äº‹åŠ¡\æ‹“å±•å­¦ä¹ \ç¬”è®°\pictures\æ¥¼+æ·±åº¦å­¦ä¹ \tensorflowæ„å»ºç¥ç»ç½‘ç»œ\æ‹Ÿæ„å»º3å±‚ç¥ç»ç½‘ç»œ-2å±‚éšå«å±‚.png)

è¾“å…¥æ•°æ®çš„ shape æ˜¯ (ğ‘,64)ï¼ŒN ä»£è¡¨æ ·æœ¬æ•°é‡ã€‚ä¸Šé¢çš„ç¥ç»ç½‘ç»œä¸€å…±æœ‰ 2 ä¸ªå…¨è¿æ¥å±‚ï¼Œå…¶ä¸­ç¬¬ä¸€å±‚å°†è¾“å…¥æ•°æ®å¤„ç†æˆ (ğ‘,30)ï¼Œæ¥ç€ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚å°†è®­ç»ƒæ•°æ®å¤„ç†æˆ (ğ‘,10)ï¼Œæœ€åç›´æ¥ä½œä¸ºè¾“å‡ºå±‚è¾“å‡ºã€‚è€Œè¾“å‡ºçš„ (ğ‘,10)æ­£å¥½ä¸ç‹¬çƒ­ç¼–ç çš„ç›®æ ‡ç›¸äº’å¯¹åº”ã€‚

ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å¯¹**éšå«å±‚è¿›è¡Œ RELU æ¿€æ´»**ï¼Œ**è¾“å‡ºå±‚ä¸€èˆ¬ä¸æ¿€æ´»**ã€‚åŒæ—¶ï¼Œè¿™ä¸€æ¬¡æˆ‘ä»¬åŒ…å«åç½®é¡¹å‚æ•°ï¼Œå¹¶ä½¿ç”¨éšæœºåˆå§‹åŒ–å¼ é‡å‚æ•°ã€‚

```python
import tensorflow as tf

class Model(object):
    def __init__(self):
        # éšæœºåˆå§‹åŒ–å¼ é‡å‚æ•°
        self.W1 = tf.Variable(tf.random.normal([64, 30]))
        self.b1 = tf.Variable(tf.random.normal([30]))
        self.W2 = tf.Variable(tf.random.normal([30, 10]))
        self.b2 = tf.Variable(tf.random.normal([10]))

    def __call__(self, x):
        x = tf.cast(x, tf.float32)  # è½¬æ¢è¾“å…¥æ•°æ®ç±»å‹
        # çº¿æ€§è®¡ç®— + RELU æ¿€æ´»
        fc1 = tf.nn.relu(tf.add(tf.matmul(x, self.W1), self.b1))  # å…¨è¿æ¥å±‚ 1
        fc2 = tf.add(tf.matmul(fc1, self.W2), self.b2)  # å…¨è¿æ¥å±‚ 2
        return fc2
```

å€¼å¾—ä¸€æçš„æ˜¯ï¼Œ`tf.cast` ä¸ä»…å¯ä»¥è½¬æ¢å¼ é‡ç±»å‹ï¼Œè¿˜å¯ä»¥ç›´æ¥å°† NumPy æ•°ç»„è½¬æ¢ä¸ºç›¸åº”ç±»å‹çš„å¸¸é‡å¼ é‡ï¼Œè®°ä½è¿™ä¸€ç‚¹ä½¿ç”¨æ—¶ä¼šéå¸¸æ–¹ä¾¿ã€‚

- äº¤å‰ç†µæŸå¤±å‡½æ•°

å®Œæˆå‰å‘ä¼ æ’­æ¨¡å‹æ„å»ºåï¼Œå¼€å§‹å®šä¹‰æŸå¤±å‡½æ•°ã€‚

äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯æˆ‘ä»¬å‰é¢å­¦è¿‡çš„å¯¹æ•°æŸå¤±å‡½æ•°ã€‚äº¤å‰ç†µä¸»è¦ç”¨äºåº¦é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒé—´çš„å·®å¼‚æ€§ä¿¡æ¯ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°ä¼šéšç€æ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡ä¸æ–­é™ä½ï¼Œè¿”å›çš„æŸå¤±å€¼è¶Šæ¥è¶Šå¤§ã€‚äº¤å‰ç†µæŸå¤±å‡½æ•°å…¬å¼å¦‚ä¸‹ï¼š
$$
H_{y^{\prime}}(y)=-\sum_{i} y_{i}^{\prime} \log \left(y_{i}\right)
$$
å…¶ä¸­ï¼Œğ‘¦ğ‘– æ˜¯é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œ yiâ€² æ˜¯å®é™…çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬é€šè¿‡**ç‹¬çƒ­ç¼–ç **å¤„ç†åçš„**æ ‡ç­¾çŸ©é˜µ**ã€‚

Softmax å‡½æ•°å…¬å¼å¦‚ä¸‹ï¼Œå®ƒå¯ä»¥å°†æ•°å€¼å¤„ç†æˆæ¦‚ç‡ã€‚
$$
\operatorname{softmax}(x)_{i}=\frac{\exp \left(x_{i}\right)}{\sum_{j} \exp \left(x_{j}\right)}
$$
ç®€å•æ¥è®²ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¨è¿æ¥å±‚çš„è¾“å‡ºé€šè¿‡è¯¥å‡½æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œè¿™åœ¨åˆ†ç±»é—®é¢˜ä¸­ç»å¸¸ç”¨åˆ°ã€‚æ¯”å¦‚ï¼Œä½ çœ‹åˆ°é¢„æµ‹ä¸€ä¸ªåŠ¨ç‰©å±äºçŒ«çš„æ¦‚ç‡ä¸º 95.8%ï¼Œåˆ™å¾ˆæœ‰å¯èƒ½æ˜¯ä½¿ç”¨äº† Softmax å‡½æ•°ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨é¸¢å°¾èŠ±åˆ†ç±»é—®é¢˜ä¸­ï¼Œå¦‚æœæœ€åå…¨è¿æ¥å±‚ç»™å‡ºäº† 3 ä¸ªè¾“å‡ºï¼Œåˆ†åˆ«æ˜¯ -1.3ï¼Œ2.6ï¼Œ-0.9ã€‚é€šè¿‡ Softmax å‡½æ•°å¤„ç†ä¹‹åï¼Œå°±å¯ä»¥å¾—åˆ° 0.02ï¼Œ0.95ï¼Œ0.03 çš„æ¦‚ç‡å€¼ã€‚ä¹Ÿå°±æ˜¯è¯´æœ‰ 95% çš„æ¦‚ç‡å±äº Versicolor ç±»åˆ«çš„é¸¢å°¾èŠ±ã€‚

ä¸ºäº†ä¾¿äºä½¿ç”¨ï¼ŒTensorFlow ä¸­ç»™å‡ºäº†**äº¤å‰ç†µæŸå¤±å‡½æ•° + Softmax å‡½æ•°**äºŒåˆä¸€ APIï¼š`tf.nn.softmax_cross_entropy_with_logits` ã€‚ä¸‹é¢æˆ‘ä»¬å°±å¯ä»¥ç›´æ¥ä½¿ç”¨è¯¥å‡½æ•°ï¼Œå…¶ä¸­ `logits` æ˜¯æ¨¡å‹è¾“å‡ºï¼Œ`labels` ä¸ºæ ·æœ¬çš„çœŸå®å€¼ã€‚è¯¥ API ä¼šè¿”å›æ¯ä¸ªæ ·æœ¬çš„æŸå¤±è®¡ç®—ç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šä½¿ç”¨ `tf.reduce_mean` æ±‚å¾—å¹³å‡å€¼ï¼Œä»è€Œå¾—åˆ°åœ¨è®­ç»ƒé›†ä¸Šçš„æŸå¤±ã€‚

```python
def loss_fn(model, x, y):
    preds = model(x)
    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=y))
```

ä¸ºä»€ä¹ˆè¦å¯¹è¾“å‡ºå€¼è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼Ÿè¿™å°±æ˜¯å› ä¸ºæˆ‘ä»¬ä¼šä½¿ç”¨ Softmax å‡½æ•°å¯¹å…¨è¿æ¥å±‚è¾“å‡ºè¿›è¡Œæ¦‚ç‡å¤„ç†ï¼Œå¹¶æœ€ç»ˆè®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚è€Œ `tf.nn.softmax_cross_entropy_with_logits` è‡ªç„¶å°±ä¼š**è¦æ±‚ä¼ å…¥ç‹¬çƒ­ç¼–ç æ•°æ®**äº†ã€‚

- ä¼˜åŒ–å™¨

æœ‰äº†æŸå¤±å‡½æ•°ï¼Œæ¥ä¸‹æ¥å°±æ˜¯å®šä¹‰ä¼˜åŒ–å™¨æ±‚å¾—å…¨å±€æŸå¤±çš„æœ€å°å€¼äº†ã€‚è¿™é‡Œæˆ‘ä»¬ä¸å†ä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼Œè€Œæ˜¯æ·±åº¦å­¦ä¹ ä¸­æ›´ä¸ºå¸¸ç”¨çš„ **Adam ä¼˜åŒ–å™¨**ã€‚Adam å®é™…ä¸Šå°±æ˜¯ä¸€ç§æ•°å­¦ä¼˜åŒ–æ–¹æ³•ï¼Œå…¶æœ€æ—©ç”± Diederik P. Kingma ç­‰äº 2014 å¹´æå‡º ã€‚Adam çš„å…¨ç§°ä¸º Adaptive Moment Estimationï¼Œå®ƒæ˜¯ä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç‡çš„ç®—æ³•ï¼Œå…¶é’ˆå¯¹æ¯ä¸€ä¸ªå‚æ•°éƒ½è®¡ç®—è‡ªé€‚åº”çš„å­¦ä¹ ç‡

```python
EPOCHS = 200  # è¿­ä»£æ­¤æ—¶
LEARNING_RATE = 0.02  # å­¦ä¹ ç‡
model = Model()  # å®ä¾‹åŒ–æ¨¡å‹ç±»
for epoch in range(EPOCHS):
    with tf.GradientTape() as tape:  # è¿½è¸ªæ¢¯åº¦
        loss = loss_fn(model, X_train, y_train)

    trainable_variables = [model.W1, model.b1, model.W2, model.b2]  # éœ€ä¼˜åŒ–å‚æ•°åˆ—è¡¨
    grads = tape.gradient(loss, trainable_variables)  # è®¡ç®—æ¢¯åº¦

    optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)  # Adam ä¼˜åŒ–å™¨
    optimizer.apply_gradients(zip(grads, trainable_variables))  # æ›´æ–°æ¢¯åº¦
    
    # æ¯ 100 ä¸ª Epoch è¾“å‡ºå„é¡¹æŒ‡æ ‡
    if epoch == 0:
        print(f'Epoch [000/{EPOCHS}], Loss: [{loss:.4f}]')
    elif (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: [{loss:.4f}]')
```

- ç½‘ç»œå‡†ç¡®ç‡

éœ€è¦å†å®šä¹‰ä¸€ä¸ªå‡†ç¡®åº¦è®¡ç®—å‡½æ•°ã€‚é¦–å…ˆ `tf.math.argmax(y, 1)` ä»çœŸå®æ ‡ç­¾ï¼ˆç‹¬çƒ­ç¼–ç ï¼‰ä¸­è¿”å›å¼ é‡è½´ä¸Šå…·æœ‰æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œä»è€Œå°† Softmax ç»“æœè½¬æ¢ä¸ºå¯¹åº”çš„å­—ç¬¦å€¼ã€‚ç„¶åä½¿ç”¨ `tf.equal` æ¯”å¯¹å„æ ·æœ¬çš„ç»“æœæ˜¯å¦æ­£ç¡®ï¼Œæœ€ç»ˆä½¿ç”¨ `reduce_mean` æ±‚å¾—å…¨éƒ¨æ ·æœ¬çš„åˆ†ç±»å‡†ç¡®åº¦ã€‚

```python
def accuracy_fn(logits, labels):
    preds = tf.argmax(logits, axis=1)  # å–å€¼æœ€å¤§çš„ç´¢å¼•ï¼Œæ­£å¥½å¯¹åº”å­—ç¬¦æ ‡ç­¾
    labels = tf.argmax(labels, axis=1)
    return tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))
```

```python
EPOCHS = 500  # è¿­ä»£æ­¤æ—¶
LEARNING_RATE = 0.02  # å­¦ä¹ ç‡
model = Model()  # å®ä¾‹åŒ–æ¨¡å‹ç±»
for epoch in range(EPOCHS):
    with tf.GradientTape() as tape:  # è¿½è¸ªæ¢¯åº¦
        loss = loss_fn(model, X_train, y_train)

    trainable_variables = [model.W1, model.b1, model.W2, model.b2]  # éœ€ä¼˜åŒ–å‚æ•°åˆ—è¡¨
    grads = tape.gradient(loss, trainable_variables)  # è®¡ç®—æ¢¯åº¦

    optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)  # Adam ä¼˜åŒ–å™¨
    optimizer.apply_gradients(zip(grads, trainable_variables))  # æ›´æ–°æ¢¯åº¦
    
    accuracy = accuracy_fn(model(X_test), y_test)  # è®¡ç®—å‡†ç¡®åº¦

    # æ¯ 100 ä¸ª Epoch è¾“å‡ºå„é¡¹æŒ‡æ ‡
    if epoch == 0:
        print(f'Epoch [000/{EPOCHS}], Accuracy: [{accuracy:.2f}], Loss: [{loss:.4f}]')
    elif (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{EPOCHS}], Accuracy: [{accuracy:.2f}], Loss: [{loss:.4f}]')
```

## TensorFlowå®ç°Mini Batchè®­ç»ƒ

> ä¹‹å‰è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œæ¯ä¸€æ¬¡å°†å…¨éƒ¨æ•°æ®ä¼ å…¥ç½‘ç»œä¸­ï¼Œå¯¹å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚
>
> ä½†å½“æ•°æ®é‡å¤ªå¤§æ—¶ï¼Œå†…å­˜æ¶ˆè€—å¤ªå¤§ã€‚
>
> è€ŒMini Batchæ–¹æ³•ï¼Œå°±æ˜¯æ•´ä¸ªæ•°æ®åˆ†æˆä¸€äº›å°æ‰¹æ¬¡æ”¾è¿›æ¨¡å‹é‡Œè¿›è¡Œè®­ç»ƒã€‚

å°æ‰¹é‡å®ç°çš„æ–¹æ³•æœ‰å¾ˆå¤šï¼Œè¿™é‡Œæˆ‘ä»¬ç»™å‡ºéå¸¸ç®€å•çš„ä¸€ç§ã€‚å®éªŒå€ŸåŠ© scikit-learn æä¾›çš„ [ *K æŠ˜äº¤å‰éªŒè¯*](https://zh.wikipedia.org/wiki/äº¤å‰é©—è­‰) æ–¹æ³•æ¥å°†æ•°æ®åˆ’åˆ†ä¸º K ä¸ª Mini Batchã€‚ç®€å•æ¥è®²ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ `sklearn.model_selection.KFold` å°†æ•°æ®åˆ’åˆ†ä¸ºç­‰é—´éš”çš„ K å—ï¼Œç„¶åæ¯æ¬¡åªé€‰æ‹© 1 å—æ•°æ®ä¼ å…¥ï¼Œæ­£å¥½ç¬¦åˆ Mini Batch çš„æ€æƒ³äº†ã€‚

```python
from sklearn.model_selection import KFold
from tqdm.notebook import tqdm

EPOCHS = 500  # è¿­ä»£æ­¤æ—¶
BATCH_SIZE = 64  # æ¯æ¬¡è¿­ä»£çš„æ‰¹é‡å¤§å°
LEARNING_RATE = 0.02  # å­¦ä¹ ç‡

model = Model()  # å®ä¾‹åŒ–æ¨¡å‹ç±»

for epoch in tqdm(range(EPOCHS)):  # è®¾å®šå…¨æ•°æ®é›†è¿­ä»£æ¬¡æ•°
    indices = np.arange(len(X_train))  # ç”Ÿæˆè®­ç»ƒæ•°æ®é•¿åº¦è§„åˆ™åºåˆ—
    np.random.shuffle(indices)  # å¯¹ç´¢å¼•åºåˆ—è¿›è¡Œæ‰“ä¹±ï¼Œä¿è¯ä¸ºéšæœºæ•°æ®åˆ’åˆ†
    batch_num = int(len(X_train)/BATCH_SIZE)  # æ ¹æ®æ‰¹é‡å¤§å°æ±‚å¾—è¦åˆ’åˆ†çš„ batch æ•°é‡
    kf = KFold(n_splits=batch_num)  # å°†æ•°æ®åˆ†å‰²æˆ batch æ•°é‡ä»½
    # KFold åˆ’åˆ†æ‰“ä¹±åçš„ç´¢å¼•åºåˆ—ï¼Œç„¶åä¾æ®ç´¢å¼•åºåˆ—ä»æ•°æ®ä¸­æŠ½å– batch æ ·æœ¬
    for _, index in kf.split(indices):
        X_batch = X_train[indices[index]]  # æŒ‰æ‰“ä¹±åçš„åºåˆ—å–å‡ºæ•°æ®
        y_batch = y_train[indices[index]]

        with tf.GradientTape() as tape:  # è¿½è¸ªæ¢¯åº¦
            loss = loss_fn(model, X_batch, y_batch)

        trainable_variables = [model.W1, model.b1,
                               model.W2, model.b2]  # éœ€ä¼˜åŒ–å‚æ•°åˆ—è¡¨
        grads = tape.gradient(loss, trainable_variables)  # è®¡ç®—æ¢¯åº¦

        optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)  # Adam ä¼˜åŒ–å™¨
        optimizer.apply_gradients(zip(grads, trainable_variables))  # æ›´æ–°æ¢¯åº¦
        accuracy = accuracy_fn(model(X_test), y_test)  # è®¡ç®—å‡†ç¡®åº¦

    # æ¯ 100 ä¸ª Epoch è¾“å‡ºå„é¡¹æŒ‡æ ‡
    if epoch == 0:
        print(f'Epoch [000/{EPOCHS}], Accuracy: [{accuracy:.2f}], Loss: [{loss:.4f}]')
    elif (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{EPOCHS}], Accuracy: [{accuracy:.2f}], Loss: [{loss:.4f}]')
```

ä¸Šé¢çš„ä»£ç ä¸­ï¼Œç”±äº KFold å¾ªç¯å¾—åˆ°çš„ `index` æ°¸è¿œæ˜¯æŒ‰é¡ºåºæ’åˆ—çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬æå‰ç”Ÿæˆäº†æ•°æ®é•¿åº¦çš„é¡ºåºåºåˆ— `indices`ï¼Œç„¶åä½¿ç”¨ `shuffle` æ‰“ä¹±è¯¥åºåˆ—ã€‚æœ€åä»æ‰“ä¹±åçš„ `indices` å–å‡ºå€¼ä½œä¸ºè®­ç»ƒæ•°æ®å– Batch çš„ç´¢å¼•ã€‚

è¿™æ ·åšçš„ç›®çš„æ˜¯ï¼Œä¿è¯æ¯ä¸€æ¬¡ Epoch è¿­ä»£ä½¿ç”¨çš„ Mini Batch çš„æ•°æ®ä¸åŒï¼Œä¸”ä¿è¯ä¸€ä¸ª Epoch èƒ½è½®å·¡å®Œå…¨éƒ¨è®­ç»ƒæ•°æ®ã€‚å¯ä»¥çœ‹åˆ°ï¼Œå°æ‰¹é‡è¿­ä»£æœ€ç»ˆçš„å‡†ç¡®ç‡ä¾æ—§ä¸é”™ï¼Œç”šè‡³ä¼šè¢«å®Œæ•´æ•°æ®é›†è¿­ä»£è¿˜è¦å¥½ã€‚åé¢è¿˜ä¼šå­¦ä¹ åˆ°ä½¿ç”¨ TensorFlow æä¾›çš„ Mini Batch æ–¹æ³•æ¥å¤„ç†æ•°æ®ã€‚



`Batch` å’Œ `Epoch`

 Batch å½“ç„¶å°±æ˜¯ Mini Batchï¼Œå³æ¯æ¬¡ä»æ•°æ®é›†ä¸­æŠ½å‡ºä¸€å°éƒ¨åˆ†ç”¨æ¥è®­ç»ƒç¥ç»ç½‘ç»œã€‚Epoch åˆ™æ˜¯å°†æ•°æ®é›†å®Œæˆè®­ç»ƒå¤šå°‘æ¬¡ï¼ŒEpoch ç”±æœ‰é™ä¸ª Batch ç»„æˆã€‚

![](D:\äº‹åŠ¡\æˆ‘çš„äº‹åŠ¡\æ‹“å±•å­¦ä¹ \ç¬”è®°\pictures\æ¥¼+æ·±åº¦å­¦ä¹ \tensorflowæ„å»ºç¥ç»ç½‘ç»œ\Epochå’ŒBatch.jpg)