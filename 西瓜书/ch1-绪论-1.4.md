[TOC]

# 归纳偏好

核心：学习算法自身的归纳偏好与问题是否匹配，往往会起到决定性作用。

## 描述

基于学习得到相关**模型**，**对应**了假设空间中的**一个假设**。

但是在**版本空间（假设集合）**中，有多个假设，会导致对应的**模型**在面临**新样本**的时候，产生**不同的输出**。

## 问题

在诸多假设中，若想要得到**理想输出**，应该采用哪一种**模型（或假设）**？

## 解决

对于一个具体的学习算法，必须要产生一个模型，而**其自身偏好**会起关键作用，学习算法**也必须有**其归纳偏好。

### 归纳偏好

机器学习算法在学习过程中对**某种类型假设**的**偏好**称为“**归纳偏好**”或“偏好”。

- 偏好1：适用情况尽可能少 与 适用情况尽可能多。
- 偏好2：特征选择，即更加`看重`某个属性。

#### 作用

原文中有下述图，有限个黑点对应**有限个训练样本点**，**穿过所有黑点**的**曲线**则是**一种假设（或模型）**。

而曲线A和曲线B则是不同假设（或模型）。

![image-20220425200636636](C:\Users\zhangcai\AppData\Roaming\Typora\typora-user-images\image-20220425200636636.png)

若是该学习算法偏好相似样本应有**相似输出**（如诸多**属性上相似**的西瓜，其**成熟程度相似**），即在**假设（模型）曲线**上应该尽量**平滑**，如此则是选择曲线A。

#### 偏好的建立

在具体问题中，**算法**的**归纳偏好**是否**与问题本身匹配**，大多数时候直接决定了算法能否**取得好的性能**。

需要一种**一般性的原则**来引导算法确立正确的偏好.

##### 奥卡姆剃刀（`Occam's razor`）

这是一种常用的、自然科学研究中**最基本的原则**，即“**若有多个假设与观察一致，则选择最简单的那个**”。

如上文之中曲线A则是更易于描述。

而“**最简单**”本身并不容易判断。

##### NFL定理（No Free Lunch Theorem）

回顾上文中的曲线A和曲线B，基于“描述简单性”，选择了较平滑的曲线A。

原文中又选用了测试样本进行测试（白色点即为测试样本点）。

![image-20220425203209409](C:\Users\zhangcai\AppData\Roaming\Typora\typora-user-images\image-20220425203209409.png)

曲线A和曲线B任一种更加优秀的可能性都存在。

该现象对应了一种结论：对于任何算法均有现象——**“聪明算法”（对应A）**和**“笨拙算法”（对应B）**对于训练集外的样本都有更加优越的场景，即它们的**期望性相同**。——即**NFL“没有免费的午餐定理”**。

- 前提

  对于上文中NFL定理，有重要前提：即所有“问题”出现的**机会相同**、或所有问题**同等重要**。

- 寓意

  其前提是**脱离实际**的，一旦脱离具体问题，空泛谈“算法更好”毫无意义。

  因此要判断是否匹配实际问题，这才会判断算法的优劣性。

  