[TOC]

# åºåˆ—åˆ°åºåˆ—æ¨¡å‹

ä¸»è¦è§£å†³ **`è¾“å…¥ä¸å®šé•¿åºåˆ—`** å’Œ **`è¾“å‡ºä¸å®šé•¿åºåˆ—`** ã€‚

## åºåˆ—é—®é¢˜å¤„ç†ç±»å‹

- 1 â†’ N ï¼šç”Ÿæˆæ¨¡å‹ï¼Œå³è¾“å…¥ä¸€ä¸ªå‘é‡ï¼Œè¾“å‡ºé•¿åº¦ä¸º N ä¸ªåºåˆ—ã€‚
- N â†’ 1 ï¼šåˆ¤åˆ«æ¨¡å‹ï¼Œå³è¾“å…¥é•¿åº¦ä¸º N ä¸ªåºåˆ—ï¼Œè¾“å‡ºä¸€ä¸ªå‘é‡ã€‚
- N â†’ N ï¼šæ ‡å‡†åºåˆ—æ¨¡å‹ï¼Œå³è¾“å…¥é•¿åº¦ä¸º N ä¸ªåºåˆ—ï¼Œè¾“å‡ºé•¿åº¦ä¸º N ä¸ªåºåˆ—ã€‚
- N â†’ M ï¼šä¸å®šé•¿åºåˆ—æ¨¡å‹ï¼Œå³è¾“å…¥é•¿åº¦ä¸º N ä¸ªåºåˆ—ï¼Œè¾“å‡ºé•¿åº¦ä¸º M ä¸ªåºåˆ—ã€‚

è€Œå¯¹äº**æ ‡å‡†å¾ªç¯ç¥ç»ç½‘ç»œ**æ¥è¯´ï¼Œå…¶åªèƒ½è§£å†³ä¸Šé¢æ‰€åˆ—å‡ºçš„å‰ä¸‰ç§é—®é¢˜ç±»å‹ï¼Œå³ **1 å¯¹ N**ï¼Œ**N å¯¹ 1**ï¼Œ**N å¯¹ N**ã€‚æ¢å¥è¯è¯´ï¼Œå°±æ˜¯å¦‚æœ**è¾“å…¥åºåˆ—å’Œè¾“å‡ºåºåˆ—ä¸ç›¸ç­‰**ï¼Œåˆ™æ— æ³•ä½¿ç”¨æ ‡å‡†çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¥å»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒKyunghyun Cho ç­‰äººå°±æå‡ºäº†**ç¼–ç æ¨¡å‹**å’Œ**è§£ç æ¨¡å‹**ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](D:\äº‹åŠ¡\æˆ‘çš„äº‹åŠ¡\æ‹“å±•å­¦ä¹ \ç¬”è®°\pictures\æ¥¼+æ·±åº¦å­¦ä¹ \NLP\encoderå’Œdecoderæ¨¡å‹.jpg)

å›¾ä¸­ï¼Œğ‘‹ğ‘– è¡¨ç¤º**è¾“å…¥åºåˆ—**ï¼Œğ‘¦ğ‘– è¡¨ç¤º**è¾“å‡ºåºåˆ—**ï¼Œğ¶ è¡¨ç¤ºè¾“å…¥åºåˆ—ç»è¿‡**ç¼–ç **åçš„**è¾“å‡ºçŠ¶æ€**ã€‚ä»ä¸Šå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¯¥æ¨¡å‹ä¸»è¦ç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œå½“æˆ‘ä»¬è¾“å…¥åºåˆ— ğ‘‹ğ‘– æ—¶ï¼Œç»è¿‡**å¾ªç¯ç¥ç»ç½‘ç»œç¼–ç **å¾—åˆ°ä¸€ä¸ª**çŠ¶æ€å‘é‡ ğ¶** ï¼Œè€Œ**è§£ç å™¨**ä¹Ÿæ˜¯ä¸€ä¸ª**å¾ªç¯ç¥ç»ç½‘ç»œ**ï¼Œå…¶é€šè¿‡ç¼–ç å™¨å¾—åˆ°çš„çŠ¶æ€ ğ¶ æ¥è¿›è¡Œ**è§£ç **ï¼Œä»è€Œå¾—åˆ°**ä¸€ç»„è¾“å‡ºåºåˆ—**ã€‚

## ä¸­è¯‘è‹±æ¡ˆä¾‹

ä¸­æ–‡ï¼šæˆ‘æœ‰ä¸€ä¸ªè‹¹æœ
è‹±æ–‡ï¼šI have a apple

![](D:\äº‹åŠ¡\æˆ‘çš„äº‹åŠ¡\æ‹“å±•å­¦ä¹ \ç¬”è®°\pictures\æ¥¼+æ·±åº¦å­¦ä¹ \NLP\æœºå™¨ç¿»è¯‘-ä¸­è¯‘è‹±æ¡ˆä¾‹.jpg)

åœ¨ä¸Šå›¾æ‰€ç¤ºä¸­ï¼Œè¦ç¿»è¯‘çš„ä¸­æ–‡ä¸º 6 ä¸ªå­—ï¼Œè¾“å…¥åºåˆ—çš„é•¿åº¦ä¸º 6ã€‚è€Œç¿»è¯‘çš„ç»“æœä¸º 4 ä¸ªå•è¯ï¼Œæ‰€ä»¥è¾“å‡ºåºåˆ—çš„é•¿åº¦ä¸º 4ã€‚å½“æˆ‘ä»¬å¾€ seq2seq æ¨¡å‹è¾“å…¥å¥å­ã€æˆ‘æœ‰ä¸€ä¸ªè‹¹æœã€‘æ—¶ï¼Œæ¨¡å‹ä¼šé€šè¿‡å¾ªç¯ç¥ç»ç½‘ç»œæå–è¾“å…¥å¥å­çš„ç‰¹å¾ï¼Œç„¶åç¼–ç æˆä¸ºä¸€ä¸ªçŠ¶æ€å‘é‡ã€‚ç„¶åå°†è¯¥å‘é‡ä½œä¸ºè§£ç å™¨çš„åˆå§‹çŠ¶æ€å€¼ï¼Œè§£ç å™¨åŒæ ·ä¹Ÿæ˜¯ä¸€ä¸ªå¾ªç¯ç¥ç»ç½‘ç»œï¼Œè€Œå¾ªç¯ç¥ç»ç½‘ç»œæ¯ä¸ªæ—¶åˆ»çš„è¾“å‡ºå°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„ç¿»è¯‘ç»“æœã€‚

# ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿ

- 3ç§æœºå™¨ç¿»è¯‘æ–¹æ³•

åŸºäºè§„åˆ™çš„æ–¹æ³•ã€‚

åŸºäºç»Ÿè®¡çš„æ–¹æ³•ã€‚

åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ã€‚

## å®ç°å°å‹æœºå™¨ç¿»è¯‘ç³»ç»Ÿ

```python
input_texts = ['æˆ‘æœ‰ä¸€ä¸ªè‹¹æœ', 'ä½ å¥½å—', 'è§åˆ°ä½ å¾ˆé«˜å…´', 'æˆ‘ç®€ç›´ä¸æ•¢ç›¸ä¿¡',
               'æˆ‘çŸ¥é“é‚£ç§æ„Ÿè§‰', 'æˆ‘çœŸçš„éå¸¸åæ‚”', 'æˆ‘ä¹Ÿè¿™æ ·ä»¥ä¸º',
               'è¿™æ ·å¯ä»¥å—', 'è¿™äº‹å¯èƒ½å‘ç”Ÿåœ¨ä»»ä½•äººèº«ä¸Š', 'æˆ‘æƒ³è¦ä¸€ä¸ªæ‰‹æœº']
output_texts = ['I have a apple', 'How are you', 'Nice to meet you',
                'I can not believe it', 'I know the feeling', 'I really regret it',
                'I thought so, too', 'Is that OK', 'It can happen to anyone', 'I want a iphone']

# ä¸€èˆ¬å¯¹äºä¸­æ–‡å¥å­è¦å…ˆåˆ†è¯
# æ­¤å¤„ç”±äºè¯­æ–™è¿‡å°‘ï¼Œä¾¿ä»¥å­—ä¸ºå•ä½ã€‚ç°åœ¨å¯¹è¾“å…¥å¥å­å‡ºç°çš„å­—è¿›è¡Œå»é‡ç»Ÿè®¡ã€‚
def count_char(input_texts):
    input_characters = set()       # ç”¨æ¥å­˜æ”¾è¾“å…¥é›†å‡ºç°çš„ä¸­æ–‡å­—
    for input_text in input_texts:  # éå†è¾“å…¥é›†çš„æ¯ä¸€ä¸ªå¥å­
        for char in input_text:    # éå†æ¯ä¸ªå¥å­çš„æ¯ä¸ªå­—
            if char not in input_characters:
                input_characters.add(char)
    return input_characters


input_characters = count_char(input_texts)

# åŒç†ï¼Œå¯¹è‹±æ–‡å¥å­ç»Ÿè®¡ã€‚
# å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ¯ä¸ªè¾“å‡ºå¥å­ä¸­éƒ½æ·»åŠ äº†å¥å­å¼€å¤´æ ‡è®°ç¬¦å· > å’Œå¥å­ç»“å°¾æ ‡è®°ç¬¦å· < ç¬¦å·ã€‚
def count_word(output_texts):
    target_characters = set()  # ç”¨æ¥å­˜æ”¾è¾“å‡ºé›†å‡ºç°çš„å•è¯
    target_texts = []          # å­˜æ”¾åŠ äº†å¥å­å¼€å¤´å’Œç»“å°¾æ ‡è®°çš„å¥å­
    for target_text in output_texts:  # éå†è¾“å‡ºé›†çš„æ¯ä¸ªå¥å­
        target_text = '> ' + target_text + ' <'
        target_texts.append(target_text)
        word_list = target_text.split(' ')  # å¯¹æ¯ä¸ªè‹±æ–‡å¥å­æŒ‰ç©ºæ ¼åˆ’åˆ†ï¼Œå¾—åˆ°æ¯ä¸ªå•è¯
        for word in word_list:             # éå†æ¯ä¸ªå•è¯
            if word not in target_characters:
                target_characters.add(word)
    return target_texts, target_characters


target_texts, target_characters = count_word(output_texts)

# ç„¶åï¼Œå®éªŒé€šè¿‡å»ºç«‹ä¸€ä¸ªå­—å…¸ï¼Œå°†å­—ç¬¦åºåˆ—åŒ–ã€‚
input_characters = sorted(list(input_characters))  # è¿™é‡Œæ’åºæ˜¯ä¸ºäº†æ¯ä¸€æ¬¡
target_characters = sorted(list(target_characters))  # æ„å»ºçš„å­—å…¸éƒ½ä¸€æ ·
# æ„å»ºå­—ç¬¦åˆ°æ•°å­—çš„å­—å…¸ï¼Œæ¯ä¸ªå­—ç¬¦å¯¹åº”ä¸€ä¸ªæ•°å­—
input_token_index = dict(
    [(char, i) for i, char in enumerate(input_characters)])
target_token_index = dict(
    [(char, i) for i, char in enumerate(target_characters)])
# åŒæ ·ï¼Œå®éªŒéœ€è¦å®šä¹‰ä¸€ä¸ªå°†æ•°å€¼è½¬åŒ–ä¸ºå­—ç¬¦çš„å­—å…¸ä»¥å¤‡åç”¨ã€‚
# æ„å»ºåå‘å­—å…¸ï¼Œæ¯ä¸ªæ•°å­—å¯¹åº”ä¸€ä¸ªå­—ç¬¦
reverse_input_char_index = dict(
    (i, char) for char, i in input_token_index.items())

reverse_target_char_index = dict(
    (i, char) for char, i in target_token_index.items())

'''
åˆ†åˆ«è®¡ç®—è¾“å…¥å­—ç¬¦å’Œè¾“å‡ºå•è¯çš„æ•°é‡ï¼Œä»¥ä¾¿åé¢å¯¹è¾“å…¥å¥å­å’Œè¾“å‡ºå¥å­è¿›è¡Œç‹¬çƒ­ç¼–ç ã€‚åŒæ—¶åˆ†åˆ«ç®—å‡ºæœ€é•¿è¾“å…¥å¥å­çš„é•¿åº¦å’Œæœ€é•¿è¾“å‡ºå¥å­çš„é•¿åº¦ã€‚
'''
num_encoder_tokens = len(input_characters)  # è¾“å…¥é›†ä¸é‡å¤çš„å­—æ•°
num_decoder_tokens = len(target_characters)  # è¾“å‡ºé›†ä¸é‡å¤çš„å•è¯æ•°
max_encoder_seq_length = max([len(txt) for txt in input_texts])  # è¾“å…¥é›†æœ€é•¿å¥å­çš„é•¿åº¦
max_decoder_seq_length = max([len(txt) for txt in target_texts])  # è¾“å‡ºé›†æœ€é•¿å¥å­çš„é•¿åº¦

'''
ç„¶åï¼Œéœ€è¦å°†è¾“å…¥å¥å­å’Œè¾“å‡ºå¥å­éƒ½è½¬åŒ–ä¸ºå‘é‡çš„å½¢å¼ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å°†è¾“å‡ºå¥å­è½¬åŒ–ä¸ºä¸¤ä»½æ•°æ®ï¼Œä¸€ä»½ä¸ºåŸå§‹çš„è¾“å‡ºå¥å­åºåˆ—ï¼Œå¦ä¸€ä»½ä¸ºè¾“å‡ºå¥å­å»¶åä¸€ä¸ªæ—¶åˆ»çš„åºåˆ—ã€‚ä¸¤ä¸ªåºåˆ—åˆ†åˆ«ä½œä¸ºè§£ç å™¨çš„è¾“å…¥å’Œè¾“å‡ºã€‚
'''
import numpy as np

# åˆ›ä¸‰ä¸ªå…¨ä¸º 0 çš„ä¸‰ç»´çŸ©é˜µï¼Œç¬¬ä¸€ç»´ä¸ºæ ·æœ¬æ•°ï¼Œç¬¬äºŒç»´ä¸ºå¥æœ€å¤§å¥å­é•¿åº¦ï¼Œç¬¬ä¸‰ç»´ä¸ºæ¯ä¸ªå­—ç¬¦çš„ç‹¬çƒ­ç¼–ç ã€‚
encoder_input_data = np.zeros(
    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')
decoder_input_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')
decoder_target_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')

for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):  # éå†è¾“å…¥é›†å’Œè¾“å‡ºé›†
    for t, char in enumerate(input_text):  # éå†è¾“å…¥é›†æ¯ä¸ªå¥å­
        encoder_input_data[i, t, input_token_index[char]] = 1.  # å­—ç¬¦å¯¹åº”çš„ä½ç½®ç­‰äº 1
    for t, char in enumerate(target_text.split(' ')):  # éå†è¾“å‡ºé›†çš„æ¯ä¸ªå•è¯
        # è§£ç å™¨çš„è¾“å…¥åºåˆ—
        decoder_input_data[i, t, target_token_index[char]] = 1.
        if t > 0:
            # è§£ç å™¨çš„è¾“å‡ºåºåˆ—
            decoder_target_data[i, t - 1, target_token_index[char]] = 1.
'''
åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œdecoder_input_data è¡¨ç¤ºè§£ç å™¨çš„è¾“å…¥åºåˆ—ï¼Œä¾‹å¦‚ï¼šã€> I have a appleã€‘ã€‚è€Œ decoder_target_data åˆ™è¡¨ç¤ºè§£ç å™¨çš„è¾“å‡ºåºåˆ—ï¼Œä¾‹å¦‚ï¼šã€I have a apple <ã€‘ã€‚
'''

```

ä»¥ä¸Šæ˜¯æ•°æ®é¢„å¤„ç†éƒ¨åˆ†ï¼Œæ¥ä¸‹æ¥å¼€å§‹æ„å»ºseq2seqæ¨¡å‹ã€‚æœ¬æ¬¡å®éªŒä½¿ç”¨ TensorFlow Keras æ¥æ­å»ºæ¨¡å‹ã€‚

è®­ç»ƒ seq2seq æ¨¡å‹æ—¶ï¼Œæ¨¡å‹å¯¹è¾“å…¥çš„ä¸­æ–‡å¥å­è¿›è¡Œ**ç¼–ç **å¾—åˆ°**ä¸€ä¸ªçŠ¶æ€å€¼**ï¼ŒçŠ¶æ€å€¼ä¹Ÿå³**ä¿å­˜äº†ä¸­æ–‡å¥å­çš„ä¿¡æ¯**ã€‚è€Œåœ¨è§£ç å™¨ç½‘ç»œä¸­ï¼Œå°†ç¼–ç å™¨å¾—åˆ°çš„çŠ¶æ€å€¼ä½œä¸ºè§£ç å™¨çš„åˆå§‹çŠ¶æ€å€¼è¾“å…¥ã€‚

æ­¤å¤–ï¼Œ**è¯­æ–™æ•°æ®**æ˜¯æ¯ä¸€æ¡ä¸­æ–‡å¥å­å¯¹åº”ä¸€æ¡è‹±æ–‡å¥å­ã€‚è€Œ**ä¸­æ–‡å¥å­**ä½œä¸º**ç¼–ç å™¨çš„è¾“å…¥**ï¼Œ**è‹±æ–‡å¥å­**ä½œä¸º**è§£ç å™¨çš„è¾“å‡º**ã€‚ä½†åœ¨è§£ç å™¨ä¸­ï¼ŒåŒæ ·ä¹Ÿéœ€è¦è¾“å…¥ï¼Œè¿™é‡Œä½¿ç”¨**å½“å‰å•è¯ä½œä¸ºè¾“å…¥**ï¼Œé€‰æ‹©**ä¸‹ä¸€ä¸ªå•è¯ä½œä¸ºè¾“å‡º**ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](D:\äº‹åŠ¡\æˆ‘çš„äº‹åŠ¡\æ‹“å±•å­¦ä¹ \ç¬”è®°\pictures\æ¥¼+æ·±åº¦å­¦ä¹ \NLP\æœºå™¨ç¿»è¯‘-ä¸­è¯‘è‹±æ¡ˆä¾‹-encoder-decoderæ¨¡å‹.jpg)

åœ¨ä¸Šå›¾ä¸­ï¼Œè§£ç å™¨çš„ `>` ç¬¦å·è¡¨ç¤ºå¥å­çš„å¼€å¤´ï¼Œ `<` ç¬¦å·è¡¨ç¤ºå¥å­çš„ç»“å°¾ã€‚ä¹Ÿå³æ˜¯è¯´ï¼Œå¯¹äºæ•°æ®é›†ä¸­çš„æ¯ä¸ªè‹±æ–‡å¥å­ï¼Œéƒ½éœ€è¦åŠ ä¸Šå¥å­å¼€å¤´çš„æ ‡è®°ç¬¦å· `>` å’Œç»“å°¾ç¬¦å· `<`ã€‚è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬çš„**è¾“å…¥æ•°æ®**ä¸»è¦å«æœ‰**ä¸¤ä»½**ï¼Œåˆ†åˆ«æ˜¯**ä¸­æ–‡å¥å­**ã€æˆ‘æœ‰ä¸€ä¸ªè‹¹æœã€‘ï¼Œ**è‹±æ–‡å¥å­**ã€> I have a appleã€‘ï¼Œ**è¾“å‡ºå¥å­**åªæœ‰**ä¸€ä»½**ã€I have a apple <ã€‘ã€‚

```python
# æŒ‰ç…§ä¸Šå›¾æ‰€ç¤ºçš„ seq2seq æ¨¡å‹ï¼Œåˆ†åˆ«æ„å»ºç¼–ç å™¨æ¨¡å‹å’Œè§£ç å™¨æ¨¡å‹ã€‚å…ˆæ¥æ„å»ºç¼–ç å™¨æ¨¡å‹ï¼š
import tensorflow as tf

latent_dim = 256  # å¾ªç¯ç¥ç»ç½‘ç»œçš„ç¥ç»å•å…ƒæ•°

# ç¼–ç å™¨æ¨¡å‹
encoder_inputs = tf.keras.Input(shape=(None, num_encoder_tokens))  # ç¼–ç å™¨çš„è¾“å…¥
encoder = tf.keras.layers.LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)  # ç¼–ç å™¨çš„è¾“å‡º

encoder_states = [state_h, state_c]  # çŠ¶æ€å€¼
'''
è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ LSTM æ¥ä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨ï¼Œæ‰€ä»¥ç¼–ç å™¨çš„è¾“å‡ºä¸»è¦å«æœ‰ä¸¤ä¸ªå€¼ï¼Œåˆ†åˆ«æ˜¯ H å’Œ C ã€‚ç°åœ¨ä½¿ç”¨è¿™ä¸¤ä¸ªå€¼ä½œä¸ºè§£ç å™¨çš„åˆå§‹çŠ¶æ€å€¼è¾“å…¥ã€‚
'''


'''

'''
# è§£ç å™¨æ¨¡å‹
decoder_inputs = tf.keras.Input(shape=(None, num_decoder_tokens))  # è§£ç å™¨è¾“å…¥
decoder_lstm = tf.keras.layers.LSTM(
    latent_dim, return_sequences=True, return_state=True)

# åˆå§‹åŒ–è§£ç æ¨¡å‹çš„çŠ¶æ€å€¼ä¸º encoder_states
decoder_outputs, _, _ = decoder_lstm(decoder_inputs,
                                     initial_state=encoder_states)

# è¿æ¥ä¸€å±‚å…¨è¿æ¥å±‚ï¼Œå¹¶ä½¿ç”¨ Softmax æ±‚å‡ºæ¯ä¸ªæ—¶åˆ»çš„è¾“å‡º
decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)  # è§£ç å™¨è¾“å‡º
decoder_outputs

'''
æ„å»ºå¥½è§£ç å™¨ä¹‹åï¼Œç°åœ¨å°†ç¼–ç å™¨å’Œè§£ç å™¨ç»“åˆèµ·æ¥æ„æˆå®Œæ•´çš„ seq2seq æ¨¡å‹ã€‚
'''
# å®šä¹‰è®­ç»ƒæ¨¡å‹
model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.summary()
```

```
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, None, 52)]   0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, None, 32)]   0                                            
__________________________________________________________________________________________________
lstm (LSTM)                     [(None, 256), (None, 316416      input_1[0][0]                    
__________________________________________________________________________________________________
lstm_1 (LSTM)                   [(None, None, 256),  295936      input_2[0][0]                    
                                                                 lstm[0][1]                       
                                                                 lstm[0][2]                       
__________________________________________________________________________________________________
dense (Dense)                   (None, None, 32)     8224        lstm_1[0][0]                     
==================================================================================================
Total params: 620,576
Trainable params: 620,576
Non-trainable params: 0
__________________________________________________________________________________________________
```

æ¥ä¸‹æ¥ï¼Œé€‰æ‹©æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼Œç¼–è¯‘æ¨¡å‹å¹¶å®Œæˆè®­ç»ƒã€‚

```python
# å®šä¹‰ä¼˜åŒ–ç®—æ³•å’ŒæŸå¤±å‡½æ•°
model.compile(optimizer='adam', loss='categorical_crossentropy')

# è®­ç»ƒæ¨¡å‹
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=10,
          epochs=200)
```

å¯¹äºç¿»è¯‘ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„ç›®çš„æ˜¯åœ¨ç¼–ç å™¨ç«¯è¾“å‡ºä¸€ä¸ªä¸­æ–‡å¥å­ï¼Œç„¶ååœ¨è§£ç å™¨ç«¯å¾—åˆ°ä¸€ä¸ªè¾“å‡ºçš„è‹±æ–‡å¥å­ã€‚è€Œä¸Šé¢å®Œæˆäº†æ¨¡å‹çš„æ„å»ºå’Œè®­ç»ƒã€‚åœ¨æ¨¡å‹çš„æµ‹è¯•æˆ–è€…æ¨ç†ä¸­ï¼Œç”±äºä¸çŸ¥é“**è¾“å‡ºåºåˆ—çš„é•¿åº¦**ï¼Œæ‰€ä»¥è¦å°†**ç¼–ç å™¨å’Œè§£ç å™¨åˆ†å¼€**ã€‚

**å½“æ¨¡å‹è®­ç»ƒå®Œæˆ**ä¹‹åï¼Œå¾—åˆ°çš„æ˜¯ä¸€ä¸ª**ç¼–ç å™¨**å’Œä¸€ä¸ª**è§£ç å™¨**ã€‚è€Œåœ¨æµ‹è¯•æ—¶ï¼Œå…ˆå°†è¦ç¿»è¯‘çš„ä¸­æ–‡å¥å­è¾“å…¥ç¼–ç å™¨ä¸­ï¼Œç»è¿‡ç¼–ç å™¨å¾—åˆ°ä¸€ä¸ªçŠ¶æ€å‘é‡ C ã€‚

åœ¨è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬å°†è§£ç å™¨çš„çš„ç¬¬ä¸€ä¸ªæ—¶åˆ»çš„è¾“å…¥éƒ½è®¾ç½®ä¸ºå¥å­å¼€å¤´ç¬¦å· `>` ã€‚æœ€åä¸€ä¸ªæ—¶åˆ»çš„è¾“å‡ºä¸ºå¥å­ç»“å°¾ç¬¦å· `<` ã€‚å› æ­¤ï¼Œåœ¨æµ‹è¯•æ—¶ï¼Œå°†å¥å­å¼€å¤´ç¬¦å· `>` ä½œä¸ºè§£ç å™¨ç¬¬ä¸€ä¸ªæ—¶åˆ»çš„è¾“å…¥ï¼Œé¢„æµ‹å‡ºæ¥çš„å¯¹åº”è‹±æ–‡å•è¯åˆ™ä½œä¸ºä¸‹ä¸€ä¸ªæ—¶åˆ»çš„è¾“å…¥ï¼Œä¾æ¬¡å¾ªç¯ã€‚å½“è¾“å‡ºä¸ºå¥å­ç»“å°¾ç¬¦å· `<` æ—¶ï¼Œåœæ­¢å¾ªç¯ï¼Œå°†è§£ç å™¨æ‰€æœ‰çš„è¾“å‡ºè¿èµ·æ¥å¾—åˆ°ä¸€ä¸ªç¿»è¯‘å¥å­ã€‚



å…ˆæ¥å®šä¹‰ç¼–ç å™¨æ¨¡å‹ï¼Œå’Œå‰é¢æ„å»ºæ¨¡å‹æ—¶ä¸€æ ·ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ `encoder_inputs` å’Œ `encoder_states` éƒ½æ˜¯æˆ‘ä»¬å‰é¢å®šä¹‰çš„å˜é‡ã€‚

```python
# é‡æ–°å®šä¹‰ç¼–ç å™¨æ¨¡å‹
encoder_model = tf.keras.Model(encoder_inputs, encoder_states)
encoder_model.summary()

'''
è§£ç å™¨æ¨¡å‹çš„å®šä¹‰ä¹Ÿç±»ä¼¼ã€‚åŒæ · decoder_lstm å’Œ decoder_dense ä¹Ÿæ˜¯æˆ‘ä»¬å‰é¢æ‰€å®šä¹‰çš„å˜é‡æˆ–å‡½æ•°ã€‚
'''
""" é‡æ–°å®šä¹‰è§£ç å™¨æ¨¡å‹ """
decoder_state_input_h = tf.keras.Input(shape=(latent_dim,))  # è§£ç å™¨çŠ¶æ€ H è¾“å…¥
decoder_state_input_c = tf.keras.Input(shape=(latent_dim,))  # è§£ç å™¨çŠ¶æ€ C è¾“å…¥
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs)   # LSTM æ¨¡å‹è¾“å‡º

decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)   # è¿æ¥ä¸€å±‚å…¨è¿æ¥å±‚
# å®šä¹‰è§£ç å™¨æ¨¡å‹
decoder_model = tf.keras.Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs] + decoder_states)

decoder_model.summary()
```

å®šä¹‰å¥½ä¸Šé¢çš„æ¨ç†æ¨¡å‹ç»“æ„ä¹‹åï¼Œç°åœ¨å°±å¯ä»¥æ¥å¯¹**æ¨¡å‹è¿›è¡Œæ¨ç†**äº†ï¼Œå…ˆæ¥å®šä¹‰ä¸€ä¸ªé¢„æµ‹å‡½æ•°ã€‚

```python
def decode_sequence(input_seq):
    """
    decoder_dense:ä¸­æ–‡å¥å­çš„å‘é‡å½¢å¼ã€‚
    """
    # ä½¿ç”¨ç¼–ç å™¨é¢„æµ‹å‡ºçŠ¶æ€å€¼
    states_value = encoder_model.predict(input_seq)

    # æ„å»ºè§£ç å™¨çš„ç¬¬ä¸€ä¸ªæ—¶åˆ»çš„è¾“å…¥ï¼Œå³å¥å­å¼€å¤´ç¬¦å· >
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    target_seq[0, 0, target_token_index['>']] = 1.
    stop_condition = False  # è®¾ç½®åœæ­¢æ¡ä»¶
    decoded_sentence = []  # å­˜æ”¾ç»“æœ
    while not stop_condition:
        # é¢„æµ‹å‡ºè§£ç å™¨çš„è¾“å‡º
        output_tokens, h, c = decoder_model.predict(
            [target_seq] + states_value)
        # æ±‚å‡ºå¯¹åº”çš„å­—ç¬¦
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        # å¦‚æœè§£ç çš„è¾“å‡ºä¸ºå¥å­ç»“å°¾ç¬¦å· < ï¼Œåˆ™åœæ­¢é¢„æµ‹
        if (sampled_char == '<' or
                len(decoded_sentence) > max_decoder_seq_length):
            stop_condition = True
        if sampled_char != '<':
            decoded_sentence.append(sampled_char)
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.
        # æ›´æ–°çŠ¶æ€ï¼Œç”¨æ¥ç»§ç»­é€å…¥ä¸‹ä¸€ä¸ªæ—¶åˆ»
        states_value = [h, c]
    return decoded_sentence
```

åŸºäº seq2seq çš„æœºå™¨ç¿»è¯‘æ¨¡å‹æµ‹è¯•ï¼š

```python
def answer(question):
    # å°†å¥å­è½¬åŒ–ä¸ºä¸€ä¸ªæ•°å­—çŸ©é˜µ
    inseq = np.zeros((1, max_encoder_seq_length,
                      num_encoder_tokens), dtype='float32')
    for t, char in enumerate(question):
        inseq[0, t, input_token_index[char]] = 1.
    # è¾“å…¥æ¨¡å‹å¾—åˆ°è¾“å‡ºç»“æœ
    decoded_sentence = decode_sequence(inseq)
    return decoded_sentence


test_sent = 'ä½ å¥½'
result = answer(test_sent)
print('ä¸­æ–‡å¥å­ï¼š', test_sent)
print('ç¿»è¯‘ç»“æœï¼š', ' '.join(result))

'''
è¿è¡Œä¸‹é¢å•å…ƒæ ¼ä»£ç è¾“å…¥ä½ æƒ³è¦ç¿»è¯‘çš„å¥å­ï¼Œä¾‹å¦‚ã€æˆ‘å¾ˆåæ‚”ã€‘ï¼Œã€ä¸æ•¢ç›¸ä¿¡èƒ½è§åˆ°ä½ ã€‘ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¾“å…¥çš„å­—å¿…é¡»è¦åœ¨è®­ç»ƒè¯­æ–™ä¸­å‡ºç°è¿‡ï¼Œå¦åˆ™ä¼šå‡ºç°æŠ¥é”™ã€‚
'''
print('è¯·è¾“å…¥ä¸­æ–‡å¥å­ï¼ŒæŒ‰å›è½¦é”®ç»“æŸã€‚')
test_sent = input()
result = answer(test_sent)
print('ä¸­æ–‡å¥å­ï¼š', test_sent)
print('ç¿»è¯‘ç»“æœï¼š', ' '.join(result))
```

# å¯¹è¯ç³»ç»Ÿ

```python
# è¯­æ–™
input_texts = ['ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·', 'å¿ƒæƒ…ä¸å¥½å¤¸æˆ‘å‡ å¥',
               'ä½ æ˜¯', 'æœˆäº®æœ‰å¤šè¿œ', 'å—¨', 'æœ€è¿‘å¦‚ä½•',
               'ä½ å¥½å—', 'è°å‘æ˜äº†ç”µç¯æ³¡', 'ä½ ç”Ÿæ°”å—']
output = ['è²Œä¼¼è¿˜ä¸é”™å“¦', 'ä½ å”‰ç®—äº†å§', 'å°±ä¸å’Œä½ è¯´',
          'æœˆäº®ä»åœ°çƒä¸Šå¹³å‡çº¦25è‹±é‡Œ', 'æ‚¨å¥½', 'æŒºå¥½',
          'å¾ˆå¥½ï¼Œè°¢è°¢', 'æ‰˜é©¬æ–¯Â·çˆ±è¿ªç”Ÿ', 'ç”Ÿæ°”æµªè´¹ç”µ']

# å…ˆç»™è¾“å‡ºçš„å¥å­æ·»åŠ ç»“å°¾ç¬¦å· <ã€‚
output_texts = []
for target_text in output:  # éå†æ¯ä¸ªå¥å­
    target_text = target_text + '<'  # æ¯ä¸ªå¥å­éƒ½åŠ ä¸Šç»“å°¾ç¬¦å·
    output_texts.append(target_text)

output_texts[0]

# åˆ†åˆ«ç»Ÿè®¡è¾“å…¥å¥å­å’Œè¾“å‡ºå¥å­å‡ºç°çš„å­—ç¬¦æ•°ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨å‰é¢æ‰€å®šä¹‰çš„ count_char å‡½æ•°æ¥è¿›è¡Œç»Ÿè®¡ã€‚
'''
ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¯¹äºä¸­æ–‡å¥å­ï¼Œéƒ½ä¼šå…ˆå¯¹å…¶è¿›è¡Œåˆ†è¯åå†è¿›è¡Œåç»­çš„å¤„ç†ã€‚ä½†å®éªŒåªç”¨åˆ°å‡ ä¸ªå¥å­ï¼Œæ‰€ä»¥ä¸ºäº†æ–¹ä¾¿ï¼Œç›´æ¥å°†æ¯ä¸ªå­—ä½œä¸ºä¸€ä¸ªè¯æ¥å¤„ç†ã€‚ç°åœ¨å¯¹è¾“å…¥å¥å­å‡ºç°çš„å­—è¿›è¡Œå»é‡ç»Ÿè®¡ã€‚
'''
def count_char(input_texts):
    input_characters = set()       # ç”¨æ¥å­˜æ”¾è¾“å…¥é›†å‡ºç°çš„ä¸­æ–‡å­—
    for input_text in input_texts:  # éå†è¾“å…¥é›†çš„æ¯ä¸€ä¸ªå¥å­
        for char in input_text:    # éå†æ¯ä¸ªå¥å­çš„æ¯ä¸ªå­—
            if char not in input_characters:
                input_characters.add(char)
    return input_characters
input_characters = count_char(input_texts)
target_characters = count_char(output_texts)

# ä¸ä¸Šæ–‡ç›¸ä¼¼ï¼Œéœ€è¦å»ºç«‹å­—å…¸å°†æ–‡æœ¬åºåˆ—åŒ–ã€‚
input_characters = sorted(list(input_characters))  # è¿™é‡Œæ’åºæ˜¯ä¸ºäº†æ¯æ¬¡æ„å»ºçš„å­—å…¸ä¸€è‡´
target_characters = sorted(list(target_characters))
# æ„å»ºå­—ç¬¦åˆ°æ•°å­—çš„å­—å…¸
input_token_index = dict(
    [(char, i) for i, char in enumerate(input_characters)])
target_token_index = dict(
    [(char, i) for i, char in enumerate(target_characters)])
# æ„å»ºæ•°å­—åˆ°å­—ç¬¦çš„å­—å…¸
reverse_input_char_index = dict(
    (i, char) for char, i in input_token_index.items())
reverse_target_char_index = dict(
    (i, char) for char, i in target_token_index.items())

# æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ†åˆ«è®¡ç®—è¾“å…¥å­—ç¬¦å’Œè¾“å‡ºå•è¯çš„æ•°é‡ï¼Œä»¥ä¾¿åé¢å¯¹è¾“å…¥å¥å­å’Œè¾“å‡ºå¥å­è¿›è¡Œç‹¬çƒ­ç¼–ç ã€‚åŒæ—¶åˆ†åˆ«ç®—å‡ºæœ€é•¿è¾“å…¥å¥å­çš„é•¿åº¦å’Œæœ€é•¿è¾“å‡ºå¥å­çš„é•¿åº¦ã€‚
num_encoder_tokens = len(input_characters)  # è¾“å…¥é›†ä¸é‡å¤çš„å­—æ•°
num_decoder_tokens = len(target_characters)  # è¾“å‡ºé›†ä¸é‡å¤çš„å­—æ•°
max_encoder_seq_length = max([len(txt) for txt in input_texts])  # è¾“å…¥é›†æœ€é•¿å¥å­çš„é•¿åº¦
max_decoder_seq_length = max([len(txt) for txt in output_texts])  # è¾“å‡ºé›†æœ€é•¿å¥å­çš„é•¿åº¦

# å¯¹æ‰€æœ‰çš„è¾“å‡ºå¥å­è¿›è¡Œå¯¹é½æ“ä½œï¼Œå¦‚æœä¸€ä¸ªå¥å­çš„é•¿åº¦å°äºæœ€å¤§é•¿åº¦ï¼Œåˆ™åœ¨è¯¥å¥å­çš„åé¢åŠ å¥å­ç»“å°¾ç¬¦å· <ã€‚
target_texts = []
for sent in output_texts:  # éå†æ¯ä¸ªå¥å­
    for i in range(len(sent), max_decoder_seq_length):
        sent += '<'  # åœ¨æ¯ä¸ªé•¿åº¦å°äºæœ€å¤§é•¿åº¦çš„å¥å­æ·»åŠ ç»“å°¾ç¬¦å·
    target_texts.append(sent)
    
# åˆ†åˆ«å¯¹è¾“å…¥å¥å­å’Œè¾“å‡ºå¥å­è¿›è¡Œç‹¬çƒ­ç¼–ç ã€‚
# åˆ›ä¸‰ä¸ªå…¨ä¸º 0 çš„ä¸‰ç»´çŸ©é˜µï¼Œç¬¬ä¸€ç»´ä¸ºæ ·æœ¬æ•°ï¼Œç¬¬äºŒç»´ä¸ºå¥æœ€å¤§å¥å­é•¿åº¦ï¼Œç¬¬ä¸‰ç»´ä¸ºæ¯ä¸ªå­—ç¬¦çš„ç‹¬çƒ­ç¼–ç ã€‚
encoder_input_data = np.zeros(
    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')

decoder_input_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')

for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):
    for t, char in enumerate(input_text):
        encoder_input_data[i, t, input_token_index[char]] = 1.

    for t, char in enumerate(target_text):
        decoder_input_data[i, t, target_token_index[char]] = 1.
   
'''
ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰å¹¶è®­ç»ƒæ¨¡å‹ã€‚è¿™é‡Œçš„æ¨¡å‹å’Œå‰é¢æ‰€å®šä¹‰çš„æœºå™¨ç¿»è¯‘æ¨¡å‹ç±»ä¼¼ï¼Œåªä¸è¿‡è¿™é‡Œéœ€è¦å°†ç¼–ç å™¨çš„çŠ¶æ€å€¼è¾“å‡ºè¿›è¡Œå˜æ¢ï¼Œä½¿å…¶å½¢çŠ¶ç”±None, latent_dim å˜ä¸º None, max_decoder_seq_length, latent_dimã€‚

latent_dim è¡¨ç¤ºç¼–ç å™¨è¾“å‡ºçŠ¶æ€å€¼çš„å‘é‡é•¿åº¦ï¼Œmax_decoder_seq_length è¡¨ç¤ºå›ç­”æ•°æ®é›†ä¸­æœ€å¤§å¥å­é•¿åº¦ã€‚ä¹Ÿå°±æ˜¯è¯´è¦å°†çŠ¶æ€å€¼ C å¤åˆ¶ max_decoder_seq_length ä»½ï¼Œä»¥ä¾¿è¾“å…¥åˆ°è§£ç å™¨ä¸­ã€‚

åœ¨å¯¹çŠ¶æ€å€¼è¿›è¡Œå˜æ¢çš„è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨åˆ°äº† Keras çš„ Lambda å‡½æ•°ï¼Œä½ å¯ä»¥é˜…è¯»  å®˜æ–¹æ–‡æ¡£ å­¦ä¹ è¯¥å‡½æ•°çš„ç”¨æ³•ã€‚
'''

# å®šä¹‰ç¼–ç å™¨æ¨¡å‹
encoder_inputs = tf.keras.Input(shape=(None, num_encoder_tokens))  # ç¼–ç å™¨è¾“å…¥
encoder = tf.keras.layers.LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_inputs)  # ç¼–ç å™¨è¾“å‡º
encoder_state = [state_h, state_c]  # çŠ¶æ€å€¼

encoder_state = tf.keras.layers.Lambda(   # åˆå¹¶çŠ¶æ€å€¼ H å’Œ C
    lambda x: tf.keras.layers.add(x))(encoder_state)
encoder_state = tf.keras.layers.Lambda(   # æ·»åŠ ä¸€ä¸ªç»´åº¦
    lambda x: tf.keras.backend.expand_dims(x, axis=1))(encoder_state)
# å¤åˆ¶å‰é¢æ‰€æ·»åŠ çš„ç»´åº¦
encoder_state3 = tf.keras.layers.Lambda(
    lambda x: tf.tile(x, multiples=[1, max_decoder_seq_length, 1]))(encoder_state)

'''
è§£ç å™¨çš„å®šä¹‰ä¹Ÿä¸ç¿»è¯‘æ¨¡å‹ç±»ä¼¼ï¼Œä½†è¿™é‡Œçš„åˆå§‹çŠ¶æ€å€¼ä¸æ˜¯ç¼–ç å™¨çš„è¾“å‡ºçŠ¶æ€å‘é‡ C ï¼Œè€Œæ˜¯è€Œæ˜¯éšæœºçš„ä¸€ä¸ªå€¼ã€‚ä¸”è§£ç å™¨æ¯ä¸ªæ—¶åˆ»çš„è¾“å…¥éƒ½å˜ä¸ºçŠ¶æ€å€¼ Cã€‚
'''
# å®šä¹‰è§£ç å™¨æ¨¡å‹
decoder_lstm = tf.keras.layers.LSTM(
    latent_dim, return_sequences=True, return_state=True)
# ç¼–ç å™¨çš„çŠ¶æ€å€¼è¾“å‡ºä½œä¸ºè§£ç å™¨çš„è¾“å…¥
decoder_outputs, _, _ = decoder_lstm(encoder_state3)
# æ·»åŠ ä¸€å±‚å…¨è¿æ¥å±‚
decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)
# æœ€åï¼Œç»“åˆç¼–ç å™¨å’Œè§£ç å™¨å¹¶æ„å»ºå‡ºæ¨¡å‹ã€‚
# å®šä¹‰æ¨¡å‹
model = tf.keras.Model(encoder_inputs, decoder_outputs)
model.summary()
# è®­ç»ƒæ¨¡å‹æ—¶ï¼Œéœ€è¦æ³¨æ„è¾“å…¥çš„æ•°æ®åªæœ‰æé—®é›†çš„å¥å­ encoder_input_dataï¼Œå› ä¸ºè§£ç å™¨ä¸éœ€è¦å›ç­”é›†ä½œä¸ºè¾“å…¥ã€‚
# å®šä¹‰ä¼˜åŒ–ç®—æ³•å’ŒæŸå¤±å‡½æ•°
model.compile(optimizer='adam', loss='categorical_crossentropy')
# è®­ç»ƒæ¨¡å‹
model.fit(encoder_input_data, decoder_input_data,
          batch_size=10,
          epochs=200)

# åŒæ ·ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºæ¨ç†çš„ç¼–ç å™¨æ¨¡å‹å’Œè§£ç å™¨æ¨¡å‹ã€‚æ¨ç†æ¨¡å‹ä¸å‰é¢æ‰€è®­ç»ƒçš„æ¨¡å‹æƒå€¼æ˜¯å…±äº«çš„ã€‚
# é‡æ–°å®šä¹‰ç¼–ç å™¨æ¨¡å‹
encoder_model = tf.keras.Model(encoder_inputs, encoder_state3)
encoder_model.summary()
# é‡æ–°å®šä¹‰è§£ç å™¨æ¨¡å‹
decoder_inputs = tf.keras.Input(shape=(None, latent_dim))
outputs, _, _ = decoder_lstm(decoder_inputs)
outputs = decoder_dense(outputs)  # å…¨è¿æ¥å±‚
decoder_model = tf.keras.Model(decoder_inputs, outputs)
decoder_model.summary()

# ç„¶åå®šä¹‰ç”¨äºè¾“å‡ºé¢„æµ‹åºåˆ—çš„å‡½æ•°ã€‚
def decode_sequence(input_seq):
    # ä½¿ç”¨ç¼–ç å™¨é¢„æµ‹å‡ºçŠ¶æ€å€¼
    states_value = encoder_model.predict(input_seq)
    # ä½¿ç”¨è§£ç å™¨é¢„æµ‹æ•°ç»“æœ
    output_tokens = decoder_model.predict(states_value)
    decoded_sentence = []  # å­˜æ”¾ç»“æœ
    # éå†ç»“æœçš„æ‰€æœ‰æ—¶åˆ»ï¼Œæ±‚å‡ºæ¯ä¸ªæ—¶åˆ»çš„è¾“å‡ºå¯¹åº”çš„å­—ç¬¦
    for i in range(max_decoder_seq_length):
        sampled_token_index = np.argmax(output_tokens[0, i, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        if sampled_char != '<':
            decoded_sentence.append(sampled_char)
    return decoded_sentence

# ä¸€åˆ‡å°±ç»ªï¼Œç°åœ¨å°±å¯ä»¥æµ‹è¯•æˆ‘ä»¬åˆšåˆšè®­ç»ƒå¥½çš„å¯¹è¯ç³»ç»Ÿäº†ã€‚
def answer(question):
    # å°†è¾“å…¥çš„å¥å­è½¬åŒ–ä¸ºå¯¹åº”çš„çŸ©é˜µ
    inseq = np.zeros((1, max_encoder_seq_length,
                      num_encoder_tokens), dtype='float32')
    for t, char in enumerate(question):
        inseq[0, t, input_token_index[char]] = 1.
    # è¾“å…¥æ¨¡å‹å¾—åˆ°ç»“æœ
    decoded_sentence = decode_sequence(inseq)
    return decoded_sentence


test_sent = 'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·'
result = answer(test_sent)
print('æé—®ï¼š', test_sent)
print('å›ç­”ï¼š', ''.join(result))

# è¿è¡Œä¸‹é¢å•å…ƒæ ¼ä»£ç è¾“å…¥ä½ æƒ³è¦ç¿»è¯‘çš„å¥å­ï¼Œä¾‹å¦‚ã€å—¨ã€‘ã€ã€ä½ å¤¸æˆ‘å‡ å¥ã€‘ã€ã€æœˆäº®å¤šè¿œã€‘ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¾“å…¥çš„å­—å¿…é¡»è¦åœ¨è®­ç»ƒè¯­æ–™ä¸­å‡ºç°è¿‡ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚
print('è¯·è¾“å…¥ä¸­æ–‡å¥å­ï¼ŒæŒ‰å›è½¦é”®ç»“æŸã€‚')
test_sent = input()
result = answer(test_sent)
print('ä¸­æ–‡å¥å­ï¼š', test_sent)
print('ç¿»è¯‘ç»“æœï¼š', ''.join(result))
```

